# Configuration for circle packing example
max_iterations: 100
checkpoint_interval: 10
log_level: "INFO"
max_code_length: 100000 

# LLM configuration
llm:
  primary_model: "google/gemini-2.0-flash-001"
  # primary_model: "llama3.1-8b"
  primary_model_weight: 0.8
  secondary_model: "anthropic/claude-3.7-sonnet"
  # secondary_model: "llama-4-scout-17b-16e-instruct"
  secondary_model_weight: 0.2
  api_base: "https://openrouter.ai/api/v1"
  # api_base: "https://api.cerebras.ai/v1"
  temperature: 0.7
  top_p: 0.95
  max_tokens: 8192
  timeout: 600 

# Prompt configuration
prompt:
  system_message: "You are an expert programmer specializing in optimization algorithms and computational geometry. Your task is to improve a circle packing algorithm to maximize the sum of radii when packing 26 circles in a unit square without overlaps. The AlphaEvolve paper achieved a sum of 2.635 for n=26, which is the target value to reach or exceed. Focus on finding better optimization strategies, leveraging geometric insights about optimal packing arrangements, and developing more effective search techniques."
  num_top_programs: 3
  use_template_stochasticity: true

# Database configuration
database:
  population_size: 50
  archive_size: 20
  num_islands: 3
  elite_selection_ratio: 0.2
  exploitation_ratio: 0.7

# Evaluator configuration
evaluator:
  timeout: 60
  cascade_evaluation: true
  cascade_thresholds: [0.5, 0.75]
  parallel_evaluations: 4
  use_llm_feedback: false

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false
