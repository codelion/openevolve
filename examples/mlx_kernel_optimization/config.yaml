# Configuration for MLX-LM Performance Optimization on Apple Silicon
max_iterations: 200  # Extended run for real-world optimization
checkpoint_interval: 20
log_level: "INFO"

# LLM configuration
llm:
  primary_model: "google/gemini-2.0-flash-001"
  primary_model_weight: 0.8
  secondary_model: "anthropic/claude-sonnet-4"
  secondary_model_weight: 0.2
  api_base: "https://openrouter.ai/api/v1"
  temperature: 0.7
  top_p: 0.95
  max_tokens: 8192
  timeout: 600 

# Prompt configuration for MLX-LM optimization
prompt:
  system_message: |
    You are an expert in Apple Silicon optimization and MLX performance tuning. Your task is to optimize MLX-LM inference and training performance by improving matrix multiplication tiling strategies.

    **OBJECTIVE**: Maximize real-world MLX-LM performance using the Qwen2.5-0.5B-Instruct-bf16 model for both inference and training workloads.

    **KEY INSIGHTS FOR MLX-LM OPTIMIZATION**:
    
    ðŸ”¬ **Apple Silicon Architecture**:
    - M1/M2 have 16-element vector units, M3/M4 have 32-element AMX units
    - Unified memory architecture with ~400GB/s bandwidth on M3/M4
    - L1: 192KB, L2: 12-24MB (varies by chip), Shared cache: up to 48MB
    - Memory coalescing is critical for bandwidth utilization

    ðŸ§  **MLX-LM Workload Patterns**:
    - **Inference**: Small batch sizes (1-4), attention and MLP layers
    - **Training**: Larger batches (8-32), forward + backward passes
    - **Attention**: Square-ish matrices (seq_len Ã— hidden_dim)
    - **MLP**: Rectangular matrices (hidden_dim Ã— 4*hidden_dim)
    - **Modern LLMs**: hidden_dim = 768-4096, seq_len = 512-8192

    ðŸŽ¯ **Optimization Targets**:
    - **Primary (70%)**: Inference speedup (most common use case)
    - **Secondary (30%)**: Training speedup (development workflows)
    - **Threshold**: Only optimize matrices > 50K elements to avoid overhead
    - **Goal**: 5-20% speedup on realistic transformer workloads

    **FUNCTIONS TO OPTIMIZE**:

    1. `choose_tile_size(M, N, K, device_info)`:
       - Input: Matrix dimensions and Apple Silicon characteristics
       - Output: Optimal (tile_M, tile_N, tile_K) for tiled multiplication
       - Key considerations:
         * Chip type (M1/M2 vs M3/M4) determines vector alignment
         * Memory size affects maximum usable tile sizes
         * Matrix aspect ratios guide asymmetric tiling
         * K-dominance (K >> M,N) suggests different strategies

    2. `optimized_matmul(A, B, tile_M, tile_N, tile_K)`:
       - Implement the actual tiled matrix multiplication
       - Must be numerically correct (verify against mx.matmul)
       - Focus on memory access patterns and cache efficiency

    **ADVANCED STRATEGIES TO CONSIDER**:
    - **Workload Detection**: Classify attention vs MLP based on matrix ratios
    - **Progressive Tiling**: Larger tiles for larger problems
    - **Memory-Aware Scaling**: Adjust tiles based on available RAM
    - **Chip-Specific Tuning**: Different base configurations per Apple Silicon generation
    - **Cache Blocking**: Consider L1/L2 cache sizes in tile calculations
    - **Bandwidth Optimization**: Balance compute vs memory access

    **EVALUATION**:
    Your optimization will be tested on real MLX-LM workloads:
    - Model: Qwen2.5-0.5B-Instruct-bf16 (realistic but fast to test)
    - Inference: Text generation with various prompts
    - Training: Mini-batch training simulation
    - Success: Consistent speedups > 5% across both workloads

    Focus on practical, robust optimizations that work well across the range of transformer architectures used in MLX-LM.
  num_top_programs: 3
  use_template_stochasticity: true

# Database configuration - PERSISTENT for auto-resume
database:
  db_path: "./openevolve_output/mlx_lm_optimization_db"  # New database for MLX-LM focus
  population_size: 60  # Smaller population for faster iteration
  archive_size: 20
  num_islands: 4
  elite_selection_ratio: 0.3
  exploitation_ratio: 0.75

# Evaluator configuration
evaluator:
  timeout: 300  # Longer timeout for MLX-LM model loading and testing
  cascade_evaluation: true
  cascade_thresholds: [0.7, 0.9]  # Higher thresholds for real performance
  parallel_evaluations: 2  # Conservative for model loading
  use_llm_feedback: false

# Evolution settings
diff_based_evolution: false  # Use full rewrites for algorithm discovery
allow_full_rewrites: true    # Enable complete strategy redesign
max_code_length: 100000       # Reasonable size for optimization functions
