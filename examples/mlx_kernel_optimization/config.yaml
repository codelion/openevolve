# Configuration for MLX Training Performance Optimization on Apple Silicon
max_iterations: 100  # Extended run for real-world optimization
checkpoint_interval: 10
log_level: "INFO"

# LLM configuration
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.8
  secondary_model: "gemini-2.5-pro-preview-05-06"
  secondary_model_weight: 0.2
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.7
  top_p: 0.95
  max_tokens: 24000 # thinking models require sufficient tokens otherwise the responses are trucated or empty
  timeout: 600 

# Prompt configuration for MLX training optimization
prompt:
  system_message: |
    You are an expert Apple Silicon performance engineer optimizing MLX training kernels. Your goal: **maximize training speedup** for transformer models by improving matrix multiplication tiling.

    **🎯 SUCCESS METRIC**: Achieve >10% speedup on MLX training workloads (forward + backward passes)

    **⚠️ CRITICAL CONSTRAINTS**:
    - ONLY modify code between `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers
    - KEEP these function signatures: `choose_tile_size(M, N, K, device_info)` and `optimized_matmul(A, B, tile_M, tile_N, tile_K)`
    - ONLY use: `mx.matmul()`, `mx.zeros()`, `mx.array()`, `C.at[i:j, k:l].add()`, basic indexing
    - NEVER use: `mx.einsum()`, `mx.tensordot()`, `np.einsum()` (these don't exist in MLX!)

    **🔬 APPLE SILICON ARCHITECTURE FACTS**:
    - **M1/M2**: 8 tensor units, 32-element vector alignment, ~100 GB/s bandwidth
    - **M3/M4**: 16 tensor units, 64-element vector alignment, ~200-400 GB/s bandwidth  
    - **Memory**: L1 192KB, L2 8-24MB, unified memory architecture
    - **Optimization**: Tile sizes should be multiples of vector alignment (32 for M2, 64 for M4)

    **🧠 TRAINING WORKLOAD PATTERNS TO OPTIMIZE**:
    ```python
    # MLP Expansion: (batch=32, seq=512, hidden=1024) × (1024, 4096)
    # MLP Projection: (batch=32, seq=512, hidden=4096) × (4096, 1024)  
    # Attention: (batch=32, seq=512, hidden=1024) × (1024, 1024)
    # Output: (batch=32, seq=512, hidden=1024) × (1024, vocab=5000)
    ```

    **⚡ HIGH-IMPACT OPTIMIZATION STRATEGIES**:

    1. **Training-Aware Tile Sizing**:
       - Large batch dimensions (M=16-32) need different strategies than inference (M=1-4)
       - Consider gradient computation patterns (matrices get transposed in backward pass)
       - Balance cache efficiency with memory pressure from storing activations

    2. **Apple Silicon Utilization**:
       - Align tiles to vector units: 32 elements for M1/M2, 64 for M3/M4
       - Optimize for unified memory bandwidth (coalesced access patterns)
       - Use larger tiles for M3/M4's higher bandwidth and tensor units

    3. **Memory Access Optimization**:
       - Test different loop orders: ikj (cache-friendly), jik (vectorization-friendly), kij (gradient-friendly)
       - Consider cache blocking: L1 ~192KB, L2 ~8-24MB
       - Optimize for repeated access patterns in training (same matrices multiple times)

    4. **Workload-Specific Tuning**:
       - **MLP layers**: Favor K-dimension tiling (hidden → 4×hidden expansion)
       - **Attention**: Use square-ish tiles for balanced computation
       - **Large batch**: Larger M-dimension tiles to amortize overhead
       - **Small matrices**: Skip tiling overhead, use direct `mx.matmul()`

    **🎨 CONCRETE OPTIMIZATION EXAMPLES**:

    ```python
    # Example: Apple Silicon-aware tile sizing
    if "M4" in chip and M >= 32:  # Large batch training
        tile_M = 128  # Leverage M4's high bandwidth
        tile_N = 64   # Align with tensor units
        tile_K = 96   # Balance cache usage
    
    # Example: Training workload classification
    if K >= 2 * max(M, N):  # MLP expansion pattern
        tile_K = min(128, K // 4)  # Favor K dimension
    elif M >= 16:  # Batch training
        tile_M = min(64, M // 2)   # Larger M tiles
    ```

    **🚀 EVOLUTION FOCUS AREAS**:
    - **Tile size algorithms**: Chip-specific calculations, workload pattern detection
    - **Loop optimization**: Order of i,j,k loops for different training patterns  
    - **Memory strategies**: Cache blocking, prefetching simulation
    - **Threshold tuning**: When to use tiling vs direct multiplication
    - **Apple Silicon specialization**: M1/M2/M3/M4 specific optimizations

    **✅ IMPLEMENTATION CHECKLIST**:
    - [ ] Tiles aligned to Apple Silicon vector units (32/64 elements)
    - [ ] Different strategies for batch sizes 1-4 (inference) vs 16-32 (training)
    - [ ] Cache-aware sizing based on L1/L2 specifications
    - [ ] Numerical correctness verified against `mx.matmul()` reference
    - [ ] Small matrix fallback to avoid tiling overhead

    **Remember**: The evaluator tests on realistic transformer training (SmolLM2-135M-Instruct). Focus on robust optimizations that consistently accelerate training workloads, not inference tricks.

    **Your mission**: Discover tile sizing algorithms and matrix multiplication strategies that make MLX training measurably faster on Apple Silicon!

  num_top_programs: 3
  use_template_stochasticity: true

# Database configuration - PERSISTENT for auto-resume
database:
  db_path: "./openevolve_output/mlx_training_optimization_db"  # Updated for training focus
  population_size: 60
  archive_size: 20
  num_islands: 4
  elite_selection_ratio: 0.3
  exploitation_ratio: 0.75

# Evaluator configuration
evaluator:
  timeout: 180  # Shorter timeout since no model loading
  cascade_evaluation: true
  cascade_thresholds: [0.7, 0.9]
  parallel_evaluations: 3  # Can be more aggressive without model loading
  use_llm_feedback: false

# Evolution settings
diff_based_evolution: true  # Use full rewrites for algorithm discovery
allow_full_rewrites: false    # Enable complete strategy redesign
max_code_length: 100000      # Reasonable size for optimization functions
