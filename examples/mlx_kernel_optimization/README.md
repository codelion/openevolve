# MLX-LM Performance Optimization with OpenEvolve

This example demonstrates using OpenEvolve to optimize real MLX-LM inference and training performance on Apple Silicon, directly measuring speedups on the `Qwen2.5-0.5B-Instruct-bf16` model.

## The New Approach: Real-World MLX-LM Optimization

Instead of synthetic matrix benchmarks, we now optimize **actual MLX-LM performance**:

‚úÖ **Real model**: Qwen2.5-0.5B-Instruct-bf16 for fast but realistic testing  
‚úÖ **Real workloads**: Text generation (inference) and training simulation  
‚úÖ **Real metrics**: End-to-end speedup measurement vs original MLX  
‚úÖ **Practical focus**: Optimize for transformer attention and MLP patterns  

## Background

MLX is the fastest inference engine on Apple Silicon:

```
Performance Comparison:
pytorch_mps    : 1.190s avg, 42.0 tokens/s
mlx            : 0.044s avg, 1135.8 tokens/s ‚≠ê 25x FASTER  
llama_cpp      : 0.316s avg, 158.0 tokens/s
```

However, MLX's matrix multiplication can be further optimized through intelligent tiling strategies that better utilize Apple Silicon's architecture.

## The Optimization Challenge

MLX-LM performance depends on efficient matrix multiplication for:

üß† **Transformer Workloads**:
- **Attention layers**: (batch√óseq_len) √ó hidden_dim √ó hidden_dim
- **MLP expansion**: (batch√óseq_len) √ó hidden_dim √ó (4√óhidden_dim)  
- **MLP projection**: (batch√óseq_len) √ó (4√óhidden_dim) √ó hidden_dim
- **Output projection**: (batch√óseq_len) √ó hidden_dim √ó vocab_size

üèóÔ∏è **Apple Silicon Architecture**:
- **M1/M2**: 16-element vector units, 12-20MB L2 cache
- **M3/M4**: 32-element AMX units, 24-48MB shared cache
- **All**: Unified memory with 200-400GB/s bandwidth
- **Challenge**: Choose optimal tile sizes for each chip and workload

## How OpenEvolve Optimizes MLX-LM

OpenEvolve evolves the `choose_tile_size()` function to:

1. **Detect workload patterns** (attention vs MLP) mathematically
2. **Adapt to Apple Silicon variant** (M1/M2/M3/M4 specific optimizations)
3. **Balance memory hierarchy** (L1/L2 cache vs unified memory bandwidth)
4. **Optimize for real transformer patterns** (not synthetic benchmarks)

## Quick Start

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Run Real MLX-LM Optimization
```bash
python ../../openevolve-run.py initial_program.py evaluator.py --config config.yaml --iterations 200
```

### Resume from Checkpoint
```bash
# If interrupted, resume with:
python ../../openevolve-run.py initial_program.py evaluator.py --config config.yaml --checkpoint ./openevolve_output/mlx_lm_optimization_db/checkpoints/checkpoint_XX --iterations 100
```

## What Gets Optimized

The evolution targets two key functions:

### 1. Tile Size Selection
```python
def choose_tile_size(M, N, K, device_info):
    """
    Choose optimal tile sizes for MLX matrix multiplication
    
    Args:
        M, N, K: Matrix dimensions (C = A @ B where A is M√óK, B is K√óN)
        device_info: Apple Silicon characteristics (chip, memory, etc.)
        
    Returns:
        (tile_M, tile_N, tile_K): Optimal tile sizes for this workload
    """
    # This function gets evolved by OpenEvolve!
    # From simple heuristics to sophisticated Apple Silicon optimization
```

### 2. Optimized Matrix Multiplication
```python
def optimized_matmul(A, B, tile_M, tile_N, tile_K):
    """
    Perform tiled matrix multiplication with optimized memory access patterns
    
    Must be numerically correct while maximizing Apple Silicon performance
    """
    # This function implements the actual tiled computation
```

## Expected Results

OpenEvolve should discover optimizations that provide:

üìà **Inference Speedup**: 5-15% faster text generation  
üìà **Training Speedup**: 10-25% faster training steps  
üéØ **Targeted Optimization**: Better performance on larger batches and longer sequences  
üèóÔ∏è **Architecture Awareness**: M3/M4 perform better than M1/M2  

## Real-World Integration

Once optimized, integrate with any MLX-LM workflow:

```python
from mlx_lm import load, generate
from mlx_lm_openevolve import enable_optimizations

# Enable OpenEvolve optimizations
enable_optimizations("./openevolve_output/best/best_program.py")

# Your existing code gets automatic speedups!
model, tokenizer = load("mlx-community/Qwen2.5-0.5B-Instruct-bf16")
text = generate(model, tokenizer, prompt="Hello world", verbose=True)
```

## Advanced: Understanding the Evaluation

The new evaluator directly measures MLX-LM performance:

### Inference Test
1. Load Qwen2.5-0.5B-Instruct-bf16 model
2. Generate text with original MLX
3. Generate same text with optimized MLX
4. Measure speedup ratio

### Training Test  
1. Create realistic training scenario with transformer layers
2. Run training steps with original MLX
3. Run same steps with optimized MLX
4. Measure training speedup ratio

### Combined Score
- **70% weight**: Inference speedup (most common use case)
- **30% weight**: Training speedup (development workflows)
- **Bonus**: Consistent optimization across both workloads

## Comparison to Synthetic Benchmarks

| **Synthetic Matrix Benchmark** | **Real MLX-LM Optimization** |
|--------------------------------|-------------------------------|
| ‚ùå Artificial matrix sizes | ‚úÖ Real transformer dimensions |
| ‚ùå GFLOPS (doesn't reflect user experience) | ‚úÖ End-to-end speedup (what users feel) |
| ‚ùå Isolated operations | ‚úÖ Full model inference/training |
| ‚ùå May not transfer to real workloads | ‚úÖ Directly optimizes actual use cases |

## Expected Evolution Discoveries

Based on transformer architecture and Apple Silicon characteristics, expect OpenEvolve to discover:

üß† **Workload Classification**:
```python
k_dominance = K / max(M, N)  # Detect MLP vs attention patterns
aspect_ratio = max(M, N) / min(M, N)  # Handle rectangular matrices
```

üîß **Chip-Specific Optimization**:
```python
if "M4" in chip:
    base_tile = 512; vector_align = 32  # Large tiles, AMX units
elif "M1" in chip:
    base_tile = 256; vector_align = 16  # Smaller tiles, older architecture
```

‚ö° **Memory Hierarchy Optimization**:
```python
# Balance L2 cache utilization vs memory bandwidth
cache_factor = device_info["l2_cache_mb"] / 16.0
memory_factor = min(2.0, device_info["memory_gb"] / 16.0)
```

This represents a significant advance from generic matrix optimization to **transformer-aware, Apple Silicon-specific, real-world performance optimization**.

## Research Impact

This approach demonstrates:

1. **Practical AI Optimization**: Directly optimizing real AI workloads, not synthetic benchmarks
2. **Hardware-Software Co-Design**: Evolving algorithms specifically for Apple Silicon architecture  
3. **Measurable User Benefit**: End-to-end speedups that users actually experience
4. **Automated Discovery**: Finding optimizations that would take experts months to develop manually

This moves beyond proof-of-concept to **production-ready AI performance optimization**.
