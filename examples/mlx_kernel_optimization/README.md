# MLX Kernel Optimization for Apple Silicon

This example demonstrates using OpenEvolve to optimize MLX matrix multiplication kernels for Apple Silicon, inspired by AlphaEvolve's optimization of TPU kernels for Google (Section 3.3.2).

## Background

We benchmarked inference engines on Apple Silicon and found:

```
Performance Results:
pytorch_mps    : 1.190s avg, 42.0 tokens/s
mlx            : 0.044s avg, 1135.8 tokens/s ⭐ FASTEST  
llama_cpp      : 0.316s avg, 158.0 tokens/s
```

**MLX is over 25x faster than PyTorch MPS!** This makes it the perfect target for kernel optimization.

## The Challenge

Matrix multiplication performance heavily depends on choosing optimal tile sizes for different matrix dimensions. The challenge is automatically determining the best tile sizes `(tile_M, tile_N, tile_K)` for:

- Different matrix shapes (transformer attention, MLP layers)
- Different Apple Silicon chips (M1/M2/M3/M4)
- Memory bandwidth constraints
- Cache characteristics

## How It Works

1. **Initial Program**: Simple tiling heuristic with fixed tile sizes
2. **Evolution Target**: Optimize the `choose_tile_size()` function using OpenEvolve
3. **Evaluation**: Measure actual MLX performance improvements
4. **Persistent Database**: Auto-resume long optimization runs

## Quick Start

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Run Optimization
```bash
python ../../openevolve-run.py initial_program.py evaluator.py --config config.yaml --iterations 100
```

### Resume from Checkpoint (Demonstrates Persistent Database)
```bash
# If interrupted, resume with:
python ../../openevolve-run.py initial_program.py evaluator.py --config config.yaml --checkpoint ./mlx_optimization_db/checkpoints/checkpoint_XX --iterations 50
```

## What Gets Optimized

The evolution targets the `choose_tile_size()` function in `initial_program.py`:

```python
def choose_tile_size(M, N, K, device_info):
    """
    Choose optimal tile sizes for MLX matrix multiplication
    - M, N, K: Matrix dimensions
    - device_info: Apple Silicon characteristics
    Returns: (tile_M, tile_N, tile_K)
    """
    # This function gets evolved by OpenEvolve!
```

## Integration with MLX-LM

Once OpenEvolve has discovered optimized tiling heuristics, you can seamlessly integrate them into any MLX-LM workflow for automatic performance improvements.

### Drop-in Integration

Your existing MLX-LM code:
```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
prompt = "Write a story about Einstein"
messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True)
text = generate(model, tokenizer, prompt=prompt, verbose=True)
```

With OpenEvolve optimizations - **just add one import**:
```python
from mlx_lm import load, generate
from mlx_lm_openevolve import enable_optimizations  # ← Add this line

enable_optimizations()  # ← And this line

# Everything else stays exactly the same!
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
prompt = "Write a story about Einstein"
messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True)
text = generate(model, tokenizer, prompt=prompt, verbose=True)
```

### What You Get

✅ **Automatic speedups** on all matrix multiplications  
✅ **Zero code changes** to your existing MLX-LM workflows  
✅ **Apple Silicon optimized** tiling discovered by evolution  
✅ **Transparent integration** - works with any MLX-LM model  
✅ **Smart fallbacks** - automatically handles edge cases  

### Performance Impact

Depending on your model and workload, expect:
- **5-20% faster inference** on transformer models
- **Better memory utilization** on Apple Silicon
- **Consistent performance** across different model sizes
- **Optimized for real workloads** (attention, MLP layers)

### How It Works

The integration:
1. **Loads optimized heuristics** from `best_program.py` (generated by OpenEvolve)
2. **Monkey-patches MLX** matrix multiplication with optimized tiling
3. **Maintains compatibility** with all existing MLX-LM code
4. **Automatically detects** when to use optimizations vs fallbacks

### Advanced Usage

```python
from mlx_lm_openevolve import enable_optimizations, get_optimization_info

# Enable with custom path to optimized kernels
enable_optimizations("./path/to/best_program.py")

# Check optimization status
info = get_optimization_info()
print(f"Optimizations enabled: {info['enabled']}")
print(f"Device: {info['device_info']}")

# Disable optimizations if needed
from mlx_lm_openevolve import disable_optimizations
disable_optimizations()
```