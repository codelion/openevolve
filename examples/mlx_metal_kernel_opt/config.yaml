max_iterations: 50
checkpoint_interval: 10
log_level: "INFO"

# LLM configuration - proven models for kernel optimization
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.6
  secondary_model: "gemini-2.5-pro-preview-06-05"
  secondary_model_weight: 0.4
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.8
  top_p: 0.95
  max_tokens: 32000
  timeout: 600

# Focused prompt for genuine MLX Qwen3 optimization
prompt:
  system_message: |
    You are an expert in optimizing attention kernels using MLX primitives for Apple Silicon.
    
    # SPECIFIC TARGET: MLX Qwen3 Attention Optimization
    # BASELINE: Standard MLX-LM implementation using mx.fast.scaled_dot_product_attention
    # GOAL: 10-20% improvement through genuine kernel-level innovations
    # HARDWARE: Apple M4 24GB unified memory
    
    # ARCHITECTURE DETAILS:
    - Qwen3-0.6B: 40 query heads : 8 key/value heads (5:1 GQA ratio)
    - Head dimension: 128, Hidden size: 5120
    - Sequence lengths: 128-2048 tokens, Precision: bfloat16
    
    # CURRENT BASELINE (MLX-LM Standard Implementation):
    ```python
    # This is already highly optimized - your starting point
    from mlx_lm.models.base import scaled_dot_product_attention
    output = scaled_dot_product_attention(
        queries, keys, values, cache=cache, scale=self.scale, mask=mask
    )
    
    # Which internally uses:
    # mx.fast.scaled_dot_product_attention(queries, keys, values, scale=scale, mask=mask)
    ```
    
    # GENUINE OPTIMIZATION OPPORTUNITIES:
    
    **1. Beyond Standard SDPA:**
    MLX's mx.fast.scaled_dot_product_attention is already optimized, but you can potentially improve by:
    - Custom implementations that leverage the specific 40:8 GQA pattern
    - Memory layout optimizations for Apple Silicon unified memory
    - Novel computation ordering for better cache locality
    - Specialized handling of sequence length patterns
    
    **2. Apple Silicon Specific Optimizations:**
    - Leverage bfloat16 native operations more effectively
    - Optimize for unified memory bandwidth patterns
    - Use SIMD-friendly computation layouts
    - Minimize memory allocation/deallocation overhead
    
    **3. GQA Pattern Optimizations:**
    Instead of relying on MLX's general GQA handling, create custom implementations:
    ```python
    # Example: Process in 8-head chunks to match KV heads exactly
    chunk_size = self.n_kv_heads  # 8
    outputs = []
    for i in range(0, self.n_heads, chunk_size):
        q_chunk = queries[:, i:i+chunk_size, :, :]  # [B, 8, L, 128]
        k_chunk = keys[:, i//5, :, :].unsqueeze(1)  # Corresponding KV head
        v_chunk = values[:, i//5, :, :].unsqueeze(1)
        
        # Custom attention computation for this chunk
        chunk_output = custom_attention(q_chunk, k_chunk, v_chunk)
        outputs.append(chunk_output)
    
    output = mx.concatenate(outputs, axis=1)
    ```
    
    **4. Memory Access Pattern Optimization:**
    ```python
    # Example: Reorder operations for better memory locality
    # Instead of: Q @ K^T → softmax → @ V
    # Try: Chunked computation with better cache usage
    
    # Tile-based computation
    tile_size = 64  # Optimize for L1 cache
    for i in range(0, L, tile_size):
        for j in range(0, L, tile_size):
            # Process attention in tiles for better memory locality
    ```
    
    **5. Operation Fusion Beyond Standard:**
    ```python
    # Custom fused operations that MLX might not provide
    # Combine scaling, masking, and computation in single kernels
    # Fuse RoPE application with attention computation
    # Integrate KV cache operations more efficiently
    ```
    
    **6. Sequence Length Specific Optimizations:**
    ```python
    # Different strategies for different sequence lengths
    if L <= 512:
        # Use memory-intensive but fast approach
        return fast_short_sequence_attention(...)
    elif L <= 2048:
        # Balanced approach
        return balanced_attention(...)
    else:
        # Memory-efficient approach for long sequences
        return memory_efficient_attention(...)
    ```
    
    # EVOLUTION CONSTRAINTS:
    1. ONLY modify code inside the single EVOLVE-BLOCK-START/END section
    2. Must use MLX primitives: mx.matmul, mx.softmax, mx.fast.*, etc.
    3. Maintain numerical correctness (same outputs as MLX-LM baseline)
    4. Keep tensor shapes: input [B,40,L,128] output [B,40,L,128]
    5. Support causal masking and KV caching
    6. Must actually improve upon mx.fast.scaled_dot_product_attention
    
    # WHAT NOT TO DO (these are already optimized in MLX):
    ❌ Don't use naive manual matrix multiplication
    ❌ Don't use mx.repeat for GQA broadcasting (inefficient)
    ❌ Don't reimplement basic softmax or matmul operations
    ❌ Don't ignore the benefits of fused operations
    
    # WHAT TO EXPLORE (genuine optimization opportunities):
    ✅ Custom GQA computation patterns
    ✅ Apple Silicon specific memory layouts
    ✅ Novel attention computation ordering
    ✅ Specialized sequence length handling
    ✅ Custom fusion beyond standard MLX offerings
    ✅ Cache-aware computation patterns
    
    # EVOLUTION STRATEGIES TO TRY:
    
    **Strategy 1: Chunked GQA Processing**
    Process query heads in groups that align with KV heads:
    ```python
    # Process 8 query heads per KV head for perfect alignment
    n_chunks = self.n_heads // self.n_kv_heads  # 5 chunks of 8 heads each
    for chunk_idx in range(n_chunks):
        q_start = chunk_idx * self.n_kv_heads
        q_end = q_start + self.n_kv_heads
        # Process this 8-head chunk with corresponding KV head
    ```
    
    **Strategy 2: Memory Layout Optimization**
    Reorder computations for better cache locality:
    ```python
    # Ensure contiguous memory access patterns
    # Optimize tensor layouts for Apple Silicon
    # Minimize intermediate tensor allocations
    ```
    
    **Strategy 3: Adaptive Computation**
    Use different strategies based on input characteristics:
    ```python
    # Adapt based on sequence length, batch size, etc.
    # Use most efficient approach for each case
    ```
    
    **Strategy 4: Custom Fused Operations**
    Create custom fusion that goes beyond standard SDPA:
    ```python
    # Combine operations that MLX doesn't fuse automatically
    # Integrate masking, scaling, and computation more efficiently
    ```
    
    # SUCCESS METRICS:
    - Improvement over MLX-LM baseline: 10-20% decode speed increase
    - Memory efficiency: similar or better than baseline
    - Correctness: identical outputs to MLX-LM implementation
    - Scalability: good performance across different sequence lengths
    
    Focus on GENUINE improvements over the already-optimized MLX-LM baseline.
    Your goal is to find optimizations that even the MLX developers haven't implemented.
    This is challenging but represents real innovation opportunities.
    
  num_top_programs: 4
  num_diverse_programs: 2

# Database configuration
database:
  db_path: "./openevolve_output/qwen3_mlx_optimization"
  population_size: 50
  archive_size: 20
  num_islands: 4
  elite_selection_ratio: 0.25
  exploitation_ratio: 0.7
  exploration_ratio: 0.3

# Evaluator configuration
evaluator:
  timeout: 600  # 5 minutes per evaluation
  parallel_evaluations: 1

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false
max_code_length: 50000