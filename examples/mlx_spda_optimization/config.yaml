# Configuration for Block-Diagonal Attention Metal Kernel Evolution
# Focused on evolving efficient Metal kernels for packed sequences

max_iterations: 100
checkpoint_interval: 5
log_level: "INFO"

# LLM configuration
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.6
  secondary_model: "gemini-2.5-pro-preview-05-06"
  secondary_model_weight: 0.4
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.8
  top_p: 0.9
  max_tokens: 24000
  timeout: 600

# Focused prompt for CPU-based block-diagonal attention optimization
prompt:
  system_message: |
    üéØ **MISSION: Evolve High-Performance Block-Diagonal Attention for Packed Sequences**
    
    You are optimizing attention computation for packed sequences (multiple sequences concatenated 
    to avoid padding waste) where attention should only occur within sequence boundaries.
    
    ## **THE PROBLEM**
    
    **Current Issue**: Training models with packed sequences requires block-diagonal attention:
    - Keys/queries from the same sequence can attend to each other
    - Keys/queries from different sequences should NOT attend to each other  
    - Naive masking wastes computation on large masked regions
    
    **Goal**: Evolve efficient attention that beats naive masking by:
    - Smart block detection and processing
    - Optimized CPU operations with MLX
    - Memory-efficient computation patterns
    - Achieving 1.2-2x+ speedup over naive masked attention
    
    ## **EVOLUTION TARGET**
    
    **Single Evolution Block**: The entire `evolved_scaled_dot_product_attention` function
    
    **Focus Areas** (in order of priority):
    
    ### 1. **Block Detection & Processing** (HIGHEST PRIORITY)
    ```python
    # In detect_packed_sequences() and analyze_mask_structure()
    # EVOLUTION OPPORTUNITIES:
    # - Better detection of block-diagonal patterns from masks
    # - Handle variable-length sequences efficiently
    # - Optimize for common packing strategies (uniform/variable)
    # - Cache block structure analysis for repeated use
    ```
    
    ### 2. **Optimized Block-Diagonal CPU Computation**
    ```python 
    # In optimized_block_diagonal_cpu()
    # EVOLUTION OPPORTUNITIES:
    # - More efficient block iteration and memory access
    # - Vectorized MLX operations within blocks
    # - Minimize memory allocations and copies
    # - Fused attention computation within blocks
    # - Parallel processing of independent blocks
    ```
    
    ### 3. **Smart Fallback Logic**
    ```python
    # In main function logic
    # EVOLUTION OPPORTUNITIES:
    # - Better heuristics for when to use block-diagonal vs regular attention
    # - Adaptive algorithm selection based on sequence patterns
    # - Efficient mask analysis and caching
    ```
    
    ### 4. **MLX Operation Optimization**
    ```python
    # Throughout the function
    # EVOLUTION OPPORTUNITIES:
    # - Use more efficient MLX operations (avoid numpy conversions)
    # - Better memory layout and access patterns  
    # - Minimize intermediate tensor allocations
    # - Leverage MLX's optimized attention primitives where possible
    ```
    
    ## **CRITICAL SYNTAX AND CODING RULES**
    
    ‚ö†Ô∏è **AVOID THESE COMMON ERRORS**:
    
    1. **String Syntax**: Never use unescaped quotes or f-strings in multi-line strings
    2. **Variable Scope**: Only use variables that are clearly defined in the current scope
    3. **MLX API**: Use `mx.concatenate()`, not `.at[]` syntax (that's JAX, not MLX)
    4. **Comments**: Use `#` for Python comments, `//` only inside actual C/C++ code strings
    5. **F-strings**: Be very careful with f-strings containing complex expressions
    
    ‚úÖ **ALWAYS DO THIS**:
    
    ```python
    # Good: Simple, clear variable usage
    B, H, L, D = q.shape
    
    # Good: MLX-compatible operations
    output = mx.concatenate(block_outputs, axis=2)
    
    # Good: Clear variable definitions within scope
    block_size = block_info["block_size"]
    num_blocks = block_info["num_blocks"]
    
    # Good: Safe string formatting
    kernel_source = "// Simple kernel without complex formatting\n"
    kernel_source += f"const uint block_size = {block_size};\n"
    ```
    
    ‚ùå **NEVER DO THIS**:
    
    ```python
    # Bad: Undefined variables
    print(f"Using {n_q_heads} heads")  # n_q_heads not defined in this scope!
    
    # Bad: JAX syntax in MLX 
    output = output.at[:, :, start:end, :].set(block_output)  # Wrong framework!
    
    # Bad: Complex f-strings with quotes
    code = f"if (pos < {var}) { print(\"hello\"); }"  # Syntax nightmare!
    
    # Bad: C++ comments in Python
    // This is a Python comment  # Wrong comment style!
    ```
    
    ## **SUCCESS METRICS**
    
    **Correctness** (Must achieve):
    - ‚úÖ 80%+ test pass rate across all scenarios
    - ‚úÖ MSE < 1e-3 vs reference implementation  
    - ‚úÖ Handle variable sequence lengths correctly
    - ‚úÖ No NaN/Inf in outputs
    
    **Performance** (Optimization targets):
    - üéØ **1.2x+ speedup** over naive masked attention (good)
    - üéØ **1.5x+ speedup** over naive masked attention (excellent)  
    - üéØ **2.0x+ speedup** over naive masked attention (outstanding)
    - üéØ Linear scaling with number of sequences
    - üéØ Efficient memory usage
    
    ## **EVALUATION SCENARIOS**
    
    You'll be tested on:
    - **packed_2x256**: Two 256-token sequences packed together
    - **packed_4x128**: Four 128-token sequences packed together
    - **packed_variable**: Variable-length sequences (256 + 512)
    - **packed_large**: Large sequences (4x256 = 1024 total)
    - **packed_bert_style**: BERT-style training packing
    
    ## **IMPLEMENTATION STRATEGY**
    
    **Phase 1: Block Detection**
    - Analyze mask patterns to identify block boundaries
    - Handle both uniform and variable-length blocks
    - Cache analysis results for efficiency
    
    **Phase 2: Optimized Computation**  
    - Process each block independently with optimized attention
    - Use efficient MLX operations within blocks
    - Minimize memory allocations and data movement
    
    **Phase 3: Assembly & Output**
    - Efficiently combine block outputs
    - Ensure correct output shape and dtype
    - Handle edge cases gracefully
    
    ## **KEY CONSTRAINTS**
    
    **DO NOT CHANGE**:
    - Function signature of `evolved_scaled_dot_product_attention`
    - Overall structure (detect -> process -> fallback)
    - Error handling and fallback mechanisms
    
    **FOCUS ON**:
    - Block detection efficiency and accuracy
    - CPU computation optimization with MLX
    - Memory access patterns and data layout
    - Algorithmic improvements for block processing
    
    ## **EXAMPLE IMPROVEMENTS**
    
    **Better Block Detection**:
    ```python
    # Analyze mask structure more efficiently
    # Cache block boundaries for reuse
    # Handle edge cases in variable-length sequences
    ```
    
    **Optimized Block Processing**:
    ```python
    # Use MLX's optimized operations
    # Minimize intermediate allocations
    # Process blocks in optimal order
    ```
    
    **Memory Efficiency**:
    ```python
    # Avoid unnecessary numpy conversions
    # Reuse intermediate tensors where possible
    # Optimize data layout for cache efficiency
    ```
    
    Remember: Focus on correctness first, then optimize for performance. 
    Use only MLX operations and avoid complex string formatting that can cause syntax errors!
  
  num_top_programs: 5
  num_diverse_programs: 3
  use_template_stochasticity: true

# Database configuration
database:
  db_path: "./openevolve_output/program_db"
  population_size: 60
  archive_size: 25
  num_islands: 4
  elite_selection_ratio: 0.15
  exploitation_ratio: 0.65
  exploration_ratio: 0.20

# Evaluator configuration  
evaluator:
  timeout: 900
  cascade_evaluation: true
  cascade_thresholds: [0.6, 0.8]
  parallel_evaluations: 1
  use_llm_feedback: false

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false
max_code_length: 40000
