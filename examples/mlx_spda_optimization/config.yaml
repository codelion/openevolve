# Configuration for MLX Custom Metal Kernel Attention Optimization
max_iterations: 100  # Increased for incremental approach
checkpoint_interval: 5
log_level: "INFO"

# LLM configuration - Use stronger models for complex Metal kernel optimization
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.6  
  secondary_model: "gemini-2.5-pro-preview-05-06" 
  secondary_model_weight: 0.4
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.7  # Lower temperature for more reliable code generation
  top_p: 0.9
  max_tokens: 24000  
  timeout: 600

# Prompt configuration
prompt:
  system_message: |
    MISSION: Incrementally evolve working Metal kernels to beat mx.fast.scaled_dot_product_attention
    
    🎯 **CURRENT IMPLEMENTATION STATUS:**
    - ✅ WORKING: Simple Metal scaling kernel (replace q * scale)
    - ❌ TODO: All other operations use reference implementation
    - 🎯 GOAL: Gradually replace reference parts with optimized Metal kernels
    
    📋 **CRITICAL SYNTAX RULES (AVOID ERRORS):**
    
    🚨 **PYTHON SYNTAX ONLY** (you are writing Python code):
    ```python
    # ✅ CORRECT Python comments
    # This is a Python comment
    
    # ❌ WRONG - C++ style comments in Python
    // This breaks Python syntax - NEVER USE
    
    # ✅ CORRECT string formatting
    source = """
        // C++ comments are OK inside Metal source strings
        uint elem = thread_position_in_grid.x;
    """
    
    # ❌ WRONG - mixing syntaxes
    source = """
        uint elem = thread_position_in_grid.x; // Comment
    """, // ❌ This comma+comment breaks Python
    ```
    
    🚨 **NEVER ACCESS NON-EXISTENT ATTRIBUTES:**
    ```python
    # ❌ WRONG - these don't exist in MLX
    array.strides     # NO!
    array.data_ptr()  # NO!
    array.device      # NO!
    
    # ✅ CORRECT - these work in MLX
    array.shape       # Yes
    array.dtype       # Yes
    array.size        # Yes
    ```
    
    🚨 **CONCRETE WORKING METAL KERNEL PATTERNS:**
    
    **1. WORKING Element-wise Kernel (PROVEN WORKING):**
    ```python
    # This pattern WORKS - use it as template
    source = """
        uint elem = thread_position_in_grid.x;
        if (elem >= input_shape[0] * input_shape[1] * input_shape[2] * input_shape[3]) {
            return;
        }
        output[elem] = input[elem] * scale_value;
    """
    
    kernel = mx.fast.metal_kernel(
        name="element_wise_op",
        input_names=["input", "scale_value"],
        output_names=["output"],
        source=source
    )
    
    # Pass scalar as array
    scale_arr = mx.array(2.0, dtype=input.dtype)
    result = kernel(
        inputs=[input, scale_arr],
        template=[("T", input.dtype)],
        output_shapes=[input.shape],
        output_dtypes=[input.dtype],
        grid=(input.size, 1, 1),
        threadgroup=(256, 1, 1)
    )[0]
    ```
    
    **2. WORKING Matrix Operation Kernel Pattern:**
    ```python
    # Template for matrix operations
    source = """
        uint tid = thread_position_in_grid.x;
        uint batch = tid / (seq_len * head_dim);
        uint remainder = tid % (seq_len * head_dim);
        uint seq_idx = remainder / head_dim;
        uint dim_idx = remainder % head_dim;
        
        if (batch >= batch_size || seq_idx >= seq_len || dim_idx >= head_dim) {
            return;
        }
        
        // Simple operation - can be evolved to more complex
        uint idx = batch * seq_len * head_dim + seq_idx * head_dim + dim_idx;
        output[idx] = input[idx] * T(2.0);
    """
    ```
    
    🎯 **INCREMENTAL EVOLUTION STRATEGY:**
    
    **Phase 1 - Element-wise Operations (START HERE):**
    - Optimize scaling: `q * scale` → custom kernel
    - Optimize masking: `scores + mask` → custom kernel  
    - Optimize activation: Replace mx.softmax with custom kernel
    
    **Phase 2 - Simple Matrix Operations:**
    - Custom transpose operations
    - Element-wise matrix operations
    - Simple reductions
    
    **Phase 3 - Complex Operations:**
    - Custom matrix multiplication kernels
    - Fused scale+matmul operations
    - Fused softmax operations
    
    **Phase 4 - Full Fusion:**
    - Fused attention kernels
    - Memory optimization
    - Advanced vectorization
    
    🛠️ **WORKING KERNEL EXAMPLES TO BUILD FROM:**
    
    **Custom Scaling Kernel (WORKS):**
    ```python
    def create_scaling_kernel():
        source = """
            uint elem = thread_position_in_grid.x;
            if (elem >= q_shape[0] * q_shape[1] * q_shape[2] * q_shape[3]) {
                return;
            }
            out[elem] = q[elem] * scale_val;
        """
        return mx.fast.metal_kernel(
            name="scale_query",
            input_names=["q", "scale_val"],
            output_names=["out"], 
            source=source
        )
    ```
    
    **Custom Element-wise Add Kernel (for masks):**
    ```python
    def create_mask_add_kernel():
        source = """
            uint elem = thread_position_in_grid.x;
            if (elem >= scores_shape[0] * scores_shape[1] * scores_shape[2] * scores_shape[3]) {
                return;
            }
            out[elem] = scores[elem] + mask[elem];
        """
        return mx.fast.metal_kernel(
            name="add_mask",
            input_names=["scores", "mask"],
            output_names=["out"],
            source=source
        )
    ```
    
    📈 **EVOLUTION PRIORITIES (DO THESE IN ORDER):**
    
    1. **Replace simple operations first** (scaling, masking)
    2. **Add more complex element-wise operations** (ReLU, exp)
    3. **Implement simple matrix operations** (transpose, broadcast)
    4. **Build up to matrix multiplication** (small tile sizes first)
    5. **Optimize memory access patterns** (coalescing, vectorization)
    6. **Fuse operations together** (scale+matmul, softmax+matmul)
    
    🚫 **CRITICAL ERRORS TO AVOID:**
    
    **Syntax Errors:**
    - Never use `//` comments in Python code (outside source strings)
    - Never mix C++ and Python syntax
    - Never use invalid Python variable names or literals
    
    **API Errors:**
    - Never access non-existent array attributes (.strides, .data_ptr)
    - Never pass invalid parameters to kernel calls
    - Always check kernel parameter validity
    
    **Logic Errors:**
    - Always check array bounds in Metal kernels
    - Never assume specific memory layouts
    - Always handle edge cases (small sequences, odd dimensions)
    
    **Performance Errors:**
    - Start simple before optimizing
    - Don't try to fuse everything at once
    - Test each optimization incrementally
    
    🎯 **SUCCESS CRITERIA:**
    - Code must compile and run without errors
    - Must maintain numerical accuracy (MSE < 1e-6)
    - Incremental performance improvements
    - Gradual replacement of reference operations
    
    **EXAMPLE EVOLUTION PATH:**
    ```python
    # Step 1: Replace q * scale with Metal kernel
    q_scaled = custom_scale_kernel(q, scale)  # ✅ Working
    
    # Step 2: Replace score masking
    masked_scores = custom_mask_kernel(scores, mask)  # Next target
    
    # Step 3: Replace matrix operations
    scores = custom_matmul_kernel(q_scaled, k_transposed)  # Future target
    
    # Step 4: Fused operations
    attention_out = fused_attention_kernel(q, k, v, scale, mask)  # End goal
    ```
    
    Start with the working scaling kernel and incrementally build complexity!
  
  num_top_programs: 5
  num_diverse_programs: 3
  use_template_stochasticity: true

# Database configuration - Tuned for incremental evolution
database:
  db_path: "./openevolve_output/program_db" 
  population_size: 80  # Smaller for more focused search
  archive_size: 30
  num_islands: 4
  elite_selection_ratio: 0.2  # Higher to preserve working solutions
  exploitation_ratio: 0.7     # Higher to build on working kernels
  exploration_ratio: 0.1      # Lower to avoid breaking working parts

# Evaluator configuration
evaluator:
  timeout: 600  # Reasonable timeout for simple kernels
  cascade_evaluation: true
  cascade_thresholds: [0.8, 0.9]  
  parallel_evaluations: 2  
  use_llm_feedback: false

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false
max_code_length: 30000  # Reasonable for incremental changes
