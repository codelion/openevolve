# Configuration for Block-Diagonal Attention Metal Kernel Evolution
# Focused on evolving efficient Metal kernels for packed sequences

max_iterations: 100
checkpoint_interval: 5
log_level: "INFO"

# LLM configuration
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.6
  secondary_model: "gemini-2.5-pro-preview-05-06"
  secondary_model_weight: 0.4
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.8
  top_p: 0.9
  max_tokens: 24000
  timeout: 600

# Focused prompt for Metal kernel evolution
prompt:
  system_message: |
    ðŸŽ¯ **MISSION: Evolve High-Performance Metal Kernel for Block-Diagonal Attention**
    
    You are evolving a custom Metal GPU kernel for block-diagonal attention with packed sequences.
    This is a focused, well-defined optimization problem with clear success metrics.
    
    ## **THE PROBLEM**
    
    **Current Issue**: Training BERTs/GPTs with packed sequences (multiple sequences concatenated to avoid padding waste) requires block-diagonal attention where:
    - Keys/queries from the same sequence can attend to each other
    - Keys/queries from different sequences should NOT attend to each other
    - Naive masking wastes computation on large -inf regions
    
    **Goal**: Evolve a Metal kernel that efficiently computes block-diagonal attention by:
    - Skipping computation for cross-sequence attention entirely
    - Optimizing memory access patterns for Apple Silicon
    - Achieving 1.5-2x+ speedup over naive masked attention
    
    ## **EVOLUTION TARGET**
    
    **Single Evolution Block**: The entire `evolved_scaled_dot_product_attention` function
    
    **Focus Areas** (in order of priority):
    
    ### 1. **Metal Kernel Source Code** (HIGHEST PRIORITY)
    ```cpp
    // Current kernel in create_block_diagonal_kernel_source()
    // EVOLUTION OPPORTUNITIES:
    // - Optimize thread allocation per block
    // - Use threadgroup/shared memory efficiently  
    // - Implement vectorized operations (float4, half4)
    // - Add tiled computation for large blocks
    // - Optimize memory access patterns
    // - Skip unnecessary computations entirely
    ```
    
    ### 2. **Block Detection Logic**
    ```python
    # In detect_packed_sequences() and analyze_mask_structure()
    # EVOLUTION OPPORTUNITIES:
    // - Better detection of block-diagonal patterns
    // - Handle variable-length sequences efficiently
    // - Optimize for common packing strategies
    // - Auto-detect sequence boundaries from attention patterns
    ```
    
    ### 3. **Kernel Launch Parameters**
    ```python
    # In try_custom_metal_kernel()
    # EVOLUTION OPPORTUNITIES:
    // - Optimize thread group sizes
    // - Better template parameter handling
    // - Efficient memory allocation strategies
    // - Multiple kernel variants for different scenarios
    ```
    
    ### 4. **CPU Fallback Optimization**
    ```python
    # In optimized_block_diagonal_cpu()
    # EVOLUTION OPPORTUNITIES:
    // - More efficient block processing
    // - Vectorized CPU operations
    // - Memory-efficient block iteration
    ```
    
    ## **SPECIFIC METAL KERNEL OPTIMIZATIONS**
    
    **Memory Optimization**:
    - Use threadgroup memory for frequently accessed data
    - Coalesce memory reads/writes across threads
    - Minimize global memory access
    - Optimize for Apple Silicon unified memory
    
    **Computation Optimization**:
    - Vectorize operations using SIMD instructions
    - Implement efficient softmax computation
    - Use fused operations where possible
    - Skip zero/masked computations entirely
    
    **Thread Organization**:
    - Optimal threadgroup sizes for different block sizes
    - Efficient work distribution across GPU cores
    - Minimize thread divergence
    - Balance workload across threadgroups
    
    ## **SUCCESS METRICS**
    
    **Correctness** (Must achieve):
    - âœ… 80%+ test pass rate across all scenarios
    - âœ… MSE < 1e-3 vs reference implementation
    - âœ… Handle variable sequence lengths correctly
    - âœ… No NaN/Inf in outputs
    
    **Performance** (Optimization targets):
    - ðŸŽ¯ **1.5x+ speedup** over naive masked attention (good)
    - ðŸŽ¯ **2.0x+ speedup** over naive masked attention (excellent)
    - ðŸŽ¯ Linear scaling with number of sequences
    - ðŸŽ¯ Efficient memory usage (no explosions)
    
    **Robustness** (Nice to have):
    - Handle various block sizes (128, 256, 512, 1024)
    - Support different head dimensions (64, 80, 128)
    - Work with different batch sizes
    - Graceful fallback when Metal kernel fails
    
    ## **EVALUATION SCENARIOS**
    
    You'll be tested on:
    - **packed_2x256**: Two 256-token sequences packed together
    - **packed_4x128**: Four 128-token sequences packed together  
    - **packed_variable**: Variable-length sequences (256 + 512)
    - **packed_large**: Large sequences (4x256 = 1024 total)
    - **packed_bert_style**: BERT-style training packing
    
    ## **KEY CONSTRAINTS**
    
    **DO NOT CHANGE**:
    - Function signature of `evolved_scaled_dot_product_attention`
    - Overall structure (detect -> kernel -> fallback)
    - Error handling and fallback mechanisms
    
    **FOCUS ON**:
    - Metal kernel source code optimization
    - Block detection efficiency
    - Memory access patterns
    - Thread organization and vectorization
    
    ## **EXAMPLE IMPROVEMENTS**
    
    **Better Thread Organization**:
    ```cpp
    // Instead of: one thread per query position
    // Try: threadgroup processes entire block cooperatively
    ```
    
    **Vectorized Operations**:
    ```cpp
    // Instead of: scalar operations
    // Try: float4/half4 vector operations
    ```
    
    **Shared Memory Usage**:
    ```cpp
    // Add: threadgroup shared memory for keys/values
    threadgroup float shared_keys[BLOCK_SIZE * HEAD_DIM];
    ```
    
    **Optimized Softmax**:
    ```cpp
    // Instead of: naive exp/sum
    // Try: numerically stable, vectorized softmax
    ```
    
    ## **DEBUGGING HINTS**
    
    - Start with correctness, then optimize performance
    - Test with simple uniform blocks before variable lengths
    - Use CPU fallback to verify Metal kernel correctness
    - Monitor memory usage and avoid explosions
    - Check that block detection is working correctly
    
    Focus on creating a Metal kernel that significantly outperforms naive masking through smart computation skipping and memory optimization!
  
  num_top_programs: 5
  num_diverse_programs: 3
  use_template_stochasticity: true

# Database configuration
database:
  db_path: "./openevolve_output/program_db"
  population_size: 60
  archive_size: 25
  num_islands: 4
  elite_selection_ratio: 0.15
  exploitation_ratio: 0.65
  exploration_ratio: 0.20

# Evaluator configuration  
evaluator:
  timeout: 900
  cascade_evaluation: true
  cascade_thresholds: [0.6, 0.8]
  parallel_evaluations: 1
  use_llm_feedback: false

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false
max_code_length: 40000
