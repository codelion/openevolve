# Enhanced Configuration for Metal Kernel Evolution
# Focus: Progressive optimization with incremental rewards and diverse exploration

max_iterations: 120
checkpoint_interval: 10
log_level: "INFO"

# LLM configuration optimized for code evolution
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.6
  secondary_model: "gemini-2.5-pro-preview-05-06"
  secondary_model_weight: 0.4
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.8
  top_p: 0.95
  max_tokens: 32000
  timeout: 900

# Structured prompt for progressive Metal kernel evolution
prompt:
  system_message: |
    # üß¨ EVOLVE HIGH-PERFORMANCE METAL ATTENTION KERNEL
    
    **MISSION**: Transform a basic Metal C++ kernel into a high-performance block-diagonal attention implementation that exploits sparsity to outperform mx.fast.scaled_dot_product_attention.
    
    ## üéØ EVOLUTION TARGET
    
    You are evolving **ONLY** the Metal C++ kernel source code within the `kernel_source` string:
    
    ```cpp
    // EVOLVE THIS KERNEL SOURCE CODE:
    template<typename T>
    [[kernel]] void block_diagonal_attention(/* fixed signature */) {
        // üî• THIS IS YOUR EVOLUTION PLAYGROUND üî•
        // Transform this from basic ‚Üí optimized ‚Üí high-performance
    }
    ```
    
    ## üìä SUCCESS FRAMEWORK
    
    **PROGRESSIVE REWARDS** - You earn points for incremental progress:
    
    ### üèÜ LEVEL 1: BASELINE IMPROVEMENT (40% of score)
    - **Target**: Beat the current/initial kernel implementation
    - **Reward**: Linear scaling for 1.1x, 1.2x, 1.5x, 2x+ speedup over baseline
    - **Why**: Incremental progress drives evolution forward
    
    ### üèÜ LEVEL 2: SPDA COMPETITION (40% of score)  
    - **Target**: Approach and beat mx.fast.scaled_dot_product_attention
    - **Reward**: Exponential bonus for beating this highly-optimized baseline
    - **Why**: Ultimate performance goal
    
    ### üèÜ LEVEL 3: SPARSITY MASTERY (20% of score)
    - **Target**: Efficiently exploit block-diagonal sparsity patterns
    - **Reward**: Bonus for consistent gains across different sparsity levels
    - **Why**: Algorithmic efficiency beyond brute-force optimization
    
    ## üöÄ OPTIMIZATION STRATEGIES
    
    ### **PHASE 1: Foundation (Early Evolution)**
    Focus on correctness and basic optimization:
    ```cpp
    // 1. Skip masked computations entirely
    if (!mask[mask_base + key_pos]) continue;
    
    // 2. Cache frequently accessed values
    T scale_val = T(scale[0]);  // Once per thread
    
    // 3. Optimize indexing calculations
    uint q_base = /* precompute base indices */;
    ```
    
    ### **PHASE 2: Memory Optimization (Mid Evolution)**
    Attack memory bottlenecks:
    ```cpp
    // 4. Vectorized memory access (HUGE WINS)
    for (uint d = 0; d < HEAD_DIM; d += 4) {
        float4 q_vec = *((device float4*)(queries + q_base + d));
        float4 k_vec = *((device float4*)(keys + k_base + d));
        score += dot(q_vec, k_vec);  // 4x fewer operations
    }
    
    // 5. Coalesced memory patterns
    // Ensure adjacent threads access adjacent memory
    
    // 6. Minimize memory bandwidth
    // Reduce redundant loads, cache in registers
    ```
    
    ### **PHASE 3: Advanced Optimization (Late Evolution)**
    Push the limits:
    ```cpp
    // 7. Fused computation passes
    // Combine score computation + softmax + output in one pass
    
    // 8. Thread workload balancing
    // Handle variable block sizes efficiently
    
    // 9. Apple Silicon specific optimizations
    // Leverage unified memory, GPU-specific features
    ```
    
    ## ‚ö° OPTIMIZATION TECHNIQUES PRIORITY
    
    **üî• CRITICAL (Must implement):**
    1. **Skip masked regions** - 50-95% compute reduction
    2. **Vectorized loads** - 2-4x memory throughput
    3. **Register optimization** - Reduce memory pressure
    
    **‚ö° HIGH IMPACT:**
    4. **Fused operations** - Reduce memory round-trips
    5. **Thread balancing** - Better GPU utilization
    6. **Coalesced access** - Memory bandwidth optimization
    
    **üîß POLISH:**
    7. **Loop unrolling** - Instruction-level optimization
    8. **Constant propagation** - Compile-time optimization
    9. **Specialized variants** - Different strategies for different sparsity
    
    ## üéÆ EVOLUTION PATTERNS
    
    **Small Mutations (60% of changes):**
    - Optimize individual loops
    - Change memory access patterns
    - Adjust vectorization
    - Cache more values
    
    **Medium Changes (30% of changes):**
    - Restructure computation order
    - Add/remove optimization passes
    - Change thread assignment
    - Fuse/unfuse operations
    
    **Large Rewrites (10% of changes):**
    - Completely different algorithmic approach
    - Novel sparsity exploitation
    - Alternative memory layouts
    
    ## üß™ TEST SCENARIOS
    
    Your kernel will be tested on:
    - **Dense (50% sparse)**: 2 large blocks - baseline performance
    - **Medium (75% sparse)**: 4 blocks - good optimization opportunity  
    - **Sparse (87% sparse)**: 8 blocks - major advantage potential
    - **Very Sparse (94% sparse)**: 16+ blocks - massive wins possible
    
    **Success Pattern**: Performance should scale with sparsity!
    
    ## üö´ CRITICAL CONSTRAINTS
    
    **NEVER CHANGE:**
    - Function signature: `block_diagonal_attention(...)`
    - Buffer parameter order: queries, keys, values, mask, scale, output
    - Template structure: `template<typename T>` 
    - Grid/threadgroup setup (handled externally)
    
    **ALWAYS MAINTAIN:**
    - Mathematical correctness of attention computation
    - Proper bounds checking for array access
    - Valid Metal C++ syntax
    
    ## üí° METAL-SPECIFIC OPTIMIZATIONS
    
    ```cpp
    // Apple Silicon advantages to exploit:
    
    // 1. Unified memory - zero-copy between CPU/GPU
    // 2. Wide SIMD units - vectorize aggressively  
    // 3. High memory bandwidth - but minimize transfers
    // 4. Threadgroup memory - use for cache optimization
    
    // Example vectorization:
    float4 q_chunk = *((device float4*)(q_ptr + d));
    float4 k_chunk = *((device float4*)(k_ptr + d));
    score += q_chunk.x*k_chunk.x + q_chunk.y*k_chunk.y + 
             q_chunk.z*k_chunk.z + q_chunk.w*k_chunk.w;
    ```
    
    ## üéØ EVOLUTION MINDSET
    
    **Think Incrementally**: Each evolution should be 5-20% better than the parent
    **Think Systematically**: Attack one bottleneck at a time
    **Think Sparsity**: Always ask "how can I skip more work?"
    **Think Metal**: Leverage Apple Silicon's unique advantages
    
    **Remember**: This is a marathon, not a sprint. Build up optimizations progressively through many evolution steps!
  
  num_top_programs: 6
  num_diverse_programs: 4
  use_template_stochasticity: true

# Enhanced database configuration for diversity and exploration
database:
  db_path: "./openevolve_output/program_db"
  population_size: 80           # Increased for more diversity
  archive_size: 40              # Larger archive for better memory
  num_islands: 6                # More islands for parallel exploration
  elite_selection_ratio: 0.15   # Slightly less elitism for more exploration
  exploitation_ratio: 0.50      # Balanced exploration vs exploitation
  exploration_ratio: 0.35       # More exploration for diverse approaches
  island_migration_rate: 0.1    # Regular migration between islands
  novelty_threshold: 0.3        # Encourage diverse solutions

# Enhanced evaluator configuration
evaluator:
  timeout: 900                  # Longer timeout for complex kernels
  cascade_evaluation: true
  cascade_thresholds: [0.5, 0.7, 0.85]  # More stages for progressive filtering
  parallel_evaluations: 2       # Utilize multiple cores
  use_llm_feedback: false
  
# Evolution settings optimized for kernel development
diff_based_evolution: true
allow_full_rewrites: false      # Allow major algorithmic changes
max_code_length: 30000         # Room for complex optimizations