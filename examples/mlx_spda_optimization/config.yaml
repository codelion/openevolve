# Configuration for MLX Block Diagonal Attention Kernel Discovery
max_iterations: 150  # More iterations for novel algorithm discovery
checkpoint_interval: 5
log_level: "INFO"

# LLM configuration - Use stronger models for algorithmic discovery
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.6  
  secondary_model: "gemini-2.5-pro-preview-05-06" 
  secondary_model_weight: 0.4
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.8  # Higher temperature for creative algorithm discovery
  top_p: 0.9
  max_tokens: 24000  
  timeout: 600

# Prompt configuration
prompt:
  system_message: |
    MISSION: Discover efficient block diagonal attention patterns for long sequence processing

    🎯 **BLOCK DIAGONAL ATTENTION DISCOVERY**
    
    You are evolving a hybrid attention system that:
    - Uses mx.fast.scaled_dot_product_attention for sequences < 512 (KEEP THIS OPTIMAL)
    - Discovers novel block diagonal attention patterns for sequences ≥ 512 (EVOLVE THIS)
    
    **STRATEGIC GOAL**: Enable 4K+ token processing with linear scaling instead of quadratic
    
    📋 **CURRENT SYSTEM ARCHITECTURE**:
    
    ```python
    def evolved_scaled_dot_product_attention(q, k, v, scale=1.0, mask=None):
        sequence_length = q.shape[2]
        
        if sequence_length < 512:
            # SHORT SEQUENCES: Use optimal implementation (DON'T TOUCH)
            return mx.fast.scaled_dot_product_attention(q, k, v, scale=scale, mask=mask)
        else:
            # LONG SEQUENCES: Use block diagonal attention (EVOLVE THIS!)
            return block_diagonal_attention(q, k, v, scale=scale, mask=mask)
    ```
    
    🎯 **EVOLUTION TARGETS** (focus on block_diagonal_attention function):
    
    **1. BLOCK PATTERN DISCOVERY** (HIGH PRIORITY):
    ```python
    # Current: Fixed 128-size blocks
    base_block_size = 128
    
    # Evolution opportunities:
    # - Adaptive block sizing based on content
    # - Hierarchical attention (blocks of blocks)
    # - Sparse block patterns (skip empty regions)
    # - Sliding window blocks with overlap
    # - Content-aware block boundaries
    ```
    
    **2. CUSTOM METAL KERNELS** (MEDIUM PRIORITY):
    ```python
    # Evolution target: Efficient block attention kernels
    source = """
        // Block-wise attention computation
        uint block_id = thread_position_in_grid.x;
        uint thread_in_block = thread_position_in_grid.y;
        
        // Optimize memory access for block patterns
        // Implement tiled computation within blocks
        // Use threadgroup memory for block data sharing
        // Vectorize operations within blocks
    """
    ```
    
    **3. ALGORITHMIC INNOVATIONS** (HIGH IMPACT):
    - **Sparse Block Attention**: Skip computation for low-attention blocks
    - **Hierarchical Blocks**: Multi-level attention (document → paragraph → sentence)
    - **Adaptive Patterns**: Change block strategy based on input characteristics
    - **Memory-Efficient Streaming**: Process very long sequences in chunks
    - **Inter-Block Communication**: Limited attention between neighboring blocks
    
    🚨 **CRITICAL CONSTRAINTS**:
    
    **DON'T BREAK THE HYBRID SYSTEM**:
    ```python
    # ✅ KEEP THIS EXACTLY AS IS (for sequences < 512):
    if sequence_length < 512:
        return mx.fast.scaled_dot_product_attention(q, k, v, scale=scale, mask=mask)
    
    # 🎯 EVOLVE THIS (for sequences ≥ 512):
    else:
        return block_diagonal_attention(q, k, v, scale=scale, mask=mask)
    ```
    
    **PRESERVE ROBUSTNESS**:
    ```python
    # Always include fallback error handling
    try:
        # Custom block diagonal implementation
        return advanced_block_attention(q, k, v, scale, mask)
    except Exception as e:
        # Fallback to simple block processing
        return simple_block_fallback(q, k, v, scale, mask)
    ```
    
    🛠️ **EVOLUTION STRATEGIES**:
    
    **Phase 1 - Block Size Optimization**:
    ```python
    # Evolve from fixed blocks to adaptive sizing
    def analyze_attention_patterns(q, k, v):
        # Discover optimal block sizes for different content types
        # Return adaptive block sizing strategy
        
    def adaptive_block_sizes(q, k, content_analysis):
        # Variable block sizes based on content density
        # Larger blocks for uniform content, smaller for complex regions
    ```
    
    **Phase 2 - Sparse Block Patterns**:
    ```python
    # Skip computation for blocks with low attention scores
    def sparse_block_selection(q, k, v):
        # Quick attention estimation to identify important blocks
        # Skip or use approximate attention for unimportant blocks
        
    def hierarchical_attention(q, k, v):
        # First pass: block-level attention scores
        # Second pass: detailed attention within important blocks
    ```
    
    **Phase 3 - Custom Block Kernels**:
    ```python
    # Implement Metal kernels optimized for block patterns
    def create_block_attention_kernel():
        source = """
            // Efficient block diagonal attention computation
            // Optimized memory access patterns for blocks
            // Vectorized operations within blocks
            // Threadgroup memory for block data sharing
        """
    ```
    
    **Phase 4 - Advanced Patterns**:
    ```python
    # Discover novel attention architectures
    # - Sliding window with memory
    # - Graph-based attention patterns  
    # - Learned sparse attention masks
    # - Multi-resolution attention hierarchies
    ```
    
    📊 **SUCCESS METRICS**:
    
    **Functionality** (Most Important):
    - Can process 2K+ token sequences without out-of-memory
    - Can process 4K+ token sequences (stretch goal)
    - Maintains reasonable attention quality within blocks
    
    **Efficiency** (Important):
    - Linear or sub-quadratic scaling with sequence length
    - Memory usage doesn't explode with long sequences
    - Execution time reasonable for long sequences (< 10s for 2K tokens)
    
    **Quality** (Acceptable Trade-off):
    - Perfect accuracy for short sequences (< 512) via hybrid system
    - Good attention quality for long sequences (some degradation acceptable)
    - Graceful quality degradation as sequences get longer
    
    🎲 **EVOLUTIONARY CREATIVITY**:
    
    **Novel Block Patterns to Explore**:
    - **Pyramid Blocks**: Increasing block sizes toward sequence end
    - **Attention-Guided Blocks**: Block boundaries based on attention patterns
    - **Sparse Diagonal**: Only compute attention for high-importance block pairs
    - **Sliding Window Blocks**: Overlapping blocks with shared computation
    - **Hierarchical Decomposition**: Recursive block subdivision
    
    **Inspiration from Other Domains**:
    - **Image Processing**: Tile-based algorithms for large images
    - **Graph Algorithms**: Sparse matrix computation techniques
    - **Database Systems**: Block-based storage and indexing
    - **Streaming Algorithms**: Processing data larger than memory
    
    🚫 **AVOID THESE MISTAKES**:
    
    **Don't break the hybrid dispatcher**:
    ```python
    # ❌ WRONG - breaks short sequence optimization
    def evolved_scaled_dot_product_attention(q, k, v, scale=1.0, mask=None):
        return always_use_custom_implementation(q, k, v, scale, mask)
    
    # ✅ CORRECT - maintains hybrid approach
    def evolved_scaled_dot_product_attention(q, k, v, scale=1.0, mask=None):
        if q.shape[2] < 512:
            return mx.fast.scaled_dot_product_attention(q, k, v, scale=scale, mask=mask)
        else:
            return block_diagonal_attention(q, k, v, scale=scale, mask=mask)
    ```
    
    **Don't optimize micro-details too early**:
    - Focus on discovering effective block patterns first
    - Optimize kernels and performance after patterns work
    - Algorithm discovery > micro-optimization
    
    **Don't sacrifice robustness**:
    - Always include fallback error handling
    - Test with various sequence lengths and configurations
    - Ensure graceful degradation for edge cases
    
    🎯 **EXAMPLE EVOLUTION DIRECTION**:
    
    ```python
    def block_diagonal_attention(q, k, v, scale=1.0, mask=None):
        # EVOLUTION STEP 1: Adaptive block sizing
        content_analysis = analyze_attention_patterns(q, k, v)
        block_sizes = adaptive_block_sizes(q, content_analysis)
        
        # EVOLUTION STEP 2: Sparse block selection
        important_blocks = sparse_block_selection(q, k, v, block_sizes)
        
        # EVOLUTION STEP 3: Efficient block computation
        block_outputs = []
        for block_info in important_blocks:
            if block_info.importance > threshold:
                # High-quality attention for important blocks
                output = detailed_block_attention(q, k, v, block_info)
            else:
                # Approximate attention for less important blocks
                output = approximate_block_attention(q, k, v, block_info)
            block_outputs.append(output)
        
        # EVOLUTION STEP 4: Combine block outputs
        return combine_block_outputs(block_outputs, original_shape=q.shape)
    ```
    
    **Remember**: You're discovering new attention algorithms, not just optimizing existing ones!
    This is about algorithmic breakthrough, not micro-optimization.
    
    Focus on making the impossible possible: processing 4K+ token sequences efficiently.
  
  num_top_programs: 6
  num_diverse_programs: 4
  use_template_stochasticity: true

# Database configuration - Optimized for algorithm discovery
database:
  db_path: "./openevolve_output/program_db" 
  population_size: 100  # Larger for diverse algorithm exploration
  archive_size: 40
  num_islands: 5
  elite_selection_ratio: 0.15  # Lower to encourage more exploration
  exploitation_ratio: 0.6      # Balanced for algorithm discovery
  exploration_ratio: 0.25     # Higher for novel pattern discovery

# Evaluator configuration - Focused on long sequence capabilities
evaluator:
  timeout: 900  # Longer timeout for long sequence processing
  cascade_evaluation: true
  cascade_thresholds: [0.6, 0.8]  # Lower first threshold for experimental algorithms
  parallel_evaluations: 1  
  use_llm_feedback: false

# Evolution settings - Optimized for algorithmic discovery
diff_based_evolution: true
allow_full_rewrites: false  # Preserve hybrid system architecture
max_code_length: 40000     # Allow for complex block attention implementations
