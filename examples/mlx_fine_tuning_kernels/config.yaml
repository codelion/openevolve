# MLX Fusion-Based Kernels Configuration
# Target: Multi-operation fusion and algorithmic improvements for beating standard MLX

max_iterations: 50
checkpoint_interval: 5
log_level: "INFO"

# LLM configuration - use powerful models for complex fusion optimizations
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.7
  secondary_model: "gemini-2.5-pro-preview-06-05"
  secondary_model_weight: 0.3
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.8
  top_p: 0.9
  max_tokens: 32000
  timeout: 600

# Detailed prompt for fusion-based optimization
prompt:
  system_message: |
    You are optimizing MLX FUSION-BASED kernels to beat standard MLX operations through 
    multi-operation fusion and algorithmic improvements.
    
    # üéØ NEW GOAL: Beat Standard MLX (Not Individual Kernels)
    Your target is to achieve 1.1x+ speedup over STANDARD MLX operation sequences
    through fusion patterns and algorithmic improvements, following Liger Kernel's approach.
    
    # üîß KEY FUSION OPPORTUNITIES
    
    **1. LoRA Weight Pre-Fusion** ‚≠ê HIGH SUCCESS PROBABILITY
    ```python
    # Instead of: 3 separate matrix multiplications
    base_out = x @ base_weight.T
    lora_out = x @ lora_a.T @ lora_b.T
    result = base_out + scale * lora_out
    
    # Target: Pre-compute combined weights (1 matmul instead of 3)
    fused_weight = base_weight + scale * (lora_b @ lora_a)
    result = x @ fused_weight.T
    ```
    
    **2. Multi-Operation Transformer Fusion**
    ```python
    # Instead of: separate RMSNorm + Attention + RMSNorm + MLP
    x = rms_norm(x, w1) -> attention(x) -> rms_norm(x, w2) -> mlp(x)
    
    # Target: Fused transformer block with shared intermediate computation
    # Combine operations to reduce kernel launches and memory transfers
    ```
    
    **3. Online/Chunked Algorithms for Memory-Bound Operations**
    ```python
    # Instead of: Full softmax materialization for large vocab
    probs = softmax(logits)  # Memory: O(vocab_size)
    loss = cross_entropy(probs, targets)
    
    # Target: Online CrossEntropy without full materialization
    loss = online_cross_entropy(logits, targets)  # Memory: O(chunk_size)
    ```
    
    **4. Memory-Efficient Attention (FlashAttention-style)**
    ```python
    # Instead of: Full attention matrix O(seq_len^2)
    scores = q @ k.T  # Materializes seq_len x seq_len
    attn = softmax(scores) @ v
    
    # Target: Chunked attention computation O(chunk_size^2)
    # Process attention in chunks to reduce peak memory
    ```
    
    **5. Training Step Fusion**
    ```python
    # Instead of: separate forward, backward, optimizer steps
    logits = model(inputs)
    loss = cross_entropy(logits, targets)
    grads = backward(loss)
    optimizer.step(grads)
    
    # Target: Fused training computation
    # Combine operations to reduce intermediate storage
    ```
    
    # üöÄ PROVEN FUSION TECHNIQUES (From Liger Kernel)
    
    **Operation Fusion**: Combine multiple operations to reduce kernel launches
    **Weight Pre-Computation**: Pre-fuse weights where possible (LoRA, layer combinations)
    **Memory Access Optimization**: Better cache utilization, chunk processing
    **Online Algorithms**: Avoid materializing large intermediate tensors
    **Chunked Computation**: Process large operations in memory-efficient chunks
    
    # üìä SUCCESS METRICS
    
    **Primary Metric**: Speedup vs Standard MLX operations
    - Target: 1.1x+ speedup over standard `nn.LayerNorm`, `nn.Linear`, etc.
    - Success: Match Liger Kernel's 20%+ improvements over standard frameworks
    
    **Secondary Metrics**:
    - Memory efficiency (reduce peak memory usage)
    - Correctness (results must match within 1e-1 tolerance)
    - Speedup vs naive implementations (should be 1.2x+)
    
    # üéñÔ∏è LIGER KERNEL SUCCESS PATTERNS TO EMULATE
    
    Liger Kernel achieved 20% speedup over PyTorch through:
    - **Multi-op fusion**: RMSNorm + scaling in single kernel
    - **Memory optimization**: In-place operations, reduced allocations
    - **Algorithmic improvements**: Online softmax, chunked computation
    - **Pre-computation**: Computing invariants once, reusing across operations
    
    Your optimizations should target similar patterns adapted for MLX.
    
    # üö´ CONSTRAINTS
    - Keep the same function signatures
    - Maintain numerical correctness (< 1e-1 difference for fusion ops)
    - Support all tensor shapes and edge cases
    - No external dependencies beyond MLX
    - Focus on FUSION not individual kernel speed
    - üö® CRITICAL: Keep code changes MINIMAL and CONCISE (under 40,000 chars)
    - NO verbose comments, examples, or redundant code
    - Use short variable names and compact formatting
    
    # üîç WHAT TO EVOLVE
    
    Focus on the `evolved_fine_tuning_kernels` function. The key operations to optimize:
    
    1. **fused_lora_linear**: Pre-compute lora_b @ lora_a, single matmul
    2. **online_cross_entropy_loss**: Chunked/online computation for large vocab
    3. **memory_efficient_attention**: Chunked attention to reduce memory O(seq_len^2)
    4. **fused_transformer_block**: Combine norm + attention + norm + mlp
    5. **fused_training_step**: Combine forward + loss + gradients + optimizer
    6. **fused_multi_layer_norm**: Multiple normalizations in single pass
    
    Evolve towards fusion patterns that MLX's compiler doesn't automatically optimize.
    The goal is operation SEQUENCES that are faster than standard MLX equivalents.
  
  num_top_programs: 6
  num_diverse_programs: 4

# Database configuration for fusion optimization
database:
  db_path: "./openevolve_output/program_db"
  population_size: 80
  archive_size: 40
  num_islands: 6
  elite_selection_ratio: 0.2
  exploitation_ratio: 0.7
  exploration_ratio: 0.3

# Evaluator configuration
evaluator:
  timeout: 900  # Longer timeout for fusion evaluations
  parallel_evaluations: 1

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false  
max_code_length: 60000
