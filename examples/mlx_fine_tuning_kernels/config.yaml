# MLX LoRA Fine-tuning Optimization Configuration
# Target: Real LoRA fine-tuning efficiency improvements while maintaining convergence

max_iterations: 50  # More iterations for breakthrough discoveries
checkpoint_interval: 5
log_level: "INFO"

# LLM configuration - use powerful models for LoRA optimization
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.7
  secondary_model: "gemini-2.5-pro-preview-06-05"
  secondary_model_weight: 0.3
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.9  # Higher creativity for breakthrough optimizations
  top_p: 0.95
  max_tokens: 32000
  timeout: 600

# Detailed prompt for LoRA optimization
prompt:
  system_message: |
    You are optimizing MLX LoRA fine-tuning implementations to achieve the same training loss
    as standard LoRA but with improved memory efficiency and/or training speed.
    
    # üéØ GOAL: Efficient LoRA Fine-tuning with Maintained Convergence
    Your target is to achieve the SAME training loss as baseline LoRA implementations
    while providing 10%+ improvements in memory usage and/or training speed.
    
    # üîß KEY OPTIMIZATION OPPORTUNITIES
    
    **1. LoRA Weight Pre-computation** ‚≠ê HIGH SUCCESS PROBABILITY
    ```python
    # Standard: 3 separate matrix multiplications per forward pass
    base_out = x @ base_weight.T
    lora_a_out = x @ lora_a.T  
    lora_b_out = lora_a_out @ lora_b.T
    result = base_out + scale * lora_b_out
    
    # Target: Pre-compute combined weights when beneficial
    if not self.training:  # During inference
        fused_weight = base_weight + scale * (lora_b @ lora_a)
        result = x @ fused_weight.T
    ```
    
    **2. Memory-Efficient Gradient Computation**
    ```python
    # Standard: Separate gradient computations
    grad_base = grad_output @ x.T
    grad_lora_b = grad_output @ lora_a_out.T  
    grad_lora_a = lora_b.T @ grad_output @ x.T
    
    # Target: Fused gradient computation to reduce memory allocations
    # Reuse intermediate tensors, optimize memory access patterns
    ```
    
    **3. Training Loop Optimization**
    ```python
    # Standard: Separate forward, loss, backward, update steps
    logits = model(inputs)
    loss = loss_fn(logits, targets)
    grads = compute_gradients(loss)
    optimizer.update(model, grads)
    
    # Target: Reduce kernel launches and memory overhead
    # Optimize for LoRA-specific gradient patterns
    ```
    
    **4. Multi-Layer LoRA Batch Processing**
    ```python
    # Standard: Apply LoRA to layers one by one
    for layer in layers:
        layer.q_proj = LoRALinear.from_linear(layer.q_proj)
        layer.v_proj = LoRALinear.from_linear(layer.v_proj)
    
    # Target: Batch LoRA operations across layers
    # Share computation, optimize memory utilization
    ```
    
    **5. Memory-Efficient Loss Computation**
    ```python
    # Standard: Full vocabulary materialization
    loss = cross_entropy(logits, targets)  # Memory: O(batch * seq * vocab)
    
    # Target: Chunked or online loss computation for large vocabularies
    # Reduce memory footprint during loss calculation
    ```
    
    **6. UNSLOTH-STYLE MLX KERNEL FUSION** üéØ PRIMARY SPEED TARGET
    ```python
    # Standard: Separate operations
    x = mx.add(input, lora_out)
    x = activation_fn(x) 
    x = mx.matmul(x, next_weight)
    
    # Target: Fused kernels using MLX primitives
    # Combine LoRA, activation, and next operation
    # Leverage mx.compile and mx.eval strategically
    ```
    
    **7. Smart Gradient Accumulation** 
    ```python
    # Standard: Individual gradient updates
    for batch in batches:
        loss = forward(batch)
        grads = backward(loss)
        optimizer.update(grads)
    
    # Target: Accumulated updates with reduced sync points
    # Batch multiple LoRA layer updates together
    ```
    
    # üöÄ UNSLOTH-INSPIRED OPTIMIZATION TECHNIQUES (Target 2x+ Speed Improvements)
    
    **üî• Flash Attention Equivalents for MLX**: Fused attention computation patterns
    **‚ö° Kernel Fusion**: Combine LoRA operations with activation functions
    **üß† Smart Gradient Accumulation**: Batch gradient updates efficiently  
    **‚≠ê Optimized MLX Operations**: Leverage mx.fast for critical paths
    **üöÄ Parameter-Efficient Updates**: Minimize optimizer state overhead
    **üíæ Memory Mapping**: Efficient tensor reuse and allocation patterns
    **üéØ Selective Computation**: Skip unnecessary ops based on LoRA rank/scale
    **üîß Mixed Precision**: Smart FP16/FP32 usage for speed without loss
    
    Current baseline shows 1.57x memory improvement but only 1.01x speed.
    FOCUS: Discover speed optimizations like unsloth's 2-5x improvements!
    
    # üìä SUCCESS METRICS
    
    **Primary Metric**: Training Loss Convergence (MUST MATCH BASELINE ¬±1%)
    - Target: Same final loss as standard LoRA implementation
    - Critical: Maintain numerical stability and gradient flow
    
    **Secondary Metrics**: Efficiency Improvements
    - Memory efficiency: 10%+ reduction in peak memory usage
    - Training speed: 10%+ improvement in tokens/second
    - Ideal: Both memory AND speed improvements
    
    # üéñÔ∏è REAL-WORLD LORA OPTIMIZATION PATTERNS
    
    Successful LoRA optimizations typically achieve:
    - **Memory reduction**: 15-30% through weight fusion and gradient optimization
    - **Speed improvement**: 10-25% through reduced kernel launches and better memory access
    - **Maintained convergence**: Critical for practical adoption
    
    Your optimizations should target similar patterns adapted for MLX.
    
    # üö´ CONSTRAINTS  
    - Keep exact function signatures from initial_program.py
    - Maintain numerical correctness (loss must match baseline within 0.01)
    - Support all LoRA configs (ranks 8-64, any scale/dropout)
    - MLX-only dependencies (mx.core, mx.nn, mx.optimizers)
    - üö® CRITICAL: Concise evolution changes (under 35,000 chars total)
    - NO verbose comments - focus on algorithmic improvements
    - Prioritize SPEED over memory (we already have 1.57x memory gain)
    - Test mx.compile, mx.eval, kernel fusion, gradient accumulation patterns
    
    # üîç WHAT TO EVOLVE - TARGET UNSLOTH-STYLE 2x+ SPEED GAINS
    
    Focus on `evolved_lora_kernels` function. Prioritize SPEED optimizations:
    
    1. **optimized_lora_fine_tuning**: Main training pipeline with kernel fusion
    2. **optimized_training_loop**: Batch gradient accumulation like unsloth  
    3. **optimized_train_step**: Fused forward/backward with mx.compile
    4. **optimized_linear_to_lora_layers**: Batched multi-layer LoRA application
    5. **optimized_evaluate**: Fast inference with weight pre-computation
    
    üéØ PRIMARY TARGETS FOR SPEED BREAKTHROUGH:
    - Leverage `mx.compile()` for hot paths (like unsloth's kernel compilation)
    - Use `mx.eval()` strategically to minimize sync points  
    - Batch operations across multiple LoRA layers simultaneously
    - Pre-compute weights when beneficial (inference mode optimization)
    - Implement gradient accumulation patterns that reduce memory allocations
    
    Current Results: 1.57x memory ‚úÖ, 1.01x speed ‚ùå
    Target: Discover 2-5x speed improvements while maintaining perfect convergence!
  
  num_top_programs: 6
  num_diverse_programs: 4

# Database configuration for LoRA optimization
database:
  db_path: "./openevolve_output/program_db"
  population_size: 80  # Larger population for more diverse explorations
  archive_size: 40
  num_islands: 4
  elite_selection_ratio: 0.20  # Less elite pressure, more exploration
  exploitation_ratio: 0.6   # Balanced exploration for breakthroughs
  exploration_ratio: 0.4

# Evaluator configuration
evaluator:
  timeout: 1200  # Longer timeout for real LoRA training
  parallel_evaluations: 1

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false  
max_code_length: 45000  # Encourage concise, focused optimizations
