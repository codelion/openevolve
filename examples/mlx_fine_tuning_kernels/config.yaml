# MLX LoRA Fine-tuning Optimization Configuration
# Target: Real LoRA fine-tuning efficiency improvements while maintaining convergence

max_iterations: 50
checkpoint_interval: 5
log_level: "INFO"

# LLM configuration - use powerful models for LoRA optimization
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.7
  secondary_model: "gemini-2.5-pro-preview-06-05"
  secondary_model_weight: 0.3
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.9
  top_p: 0.95
  max_tokens: 32000
  timeout: 600

# Detailed prompt for LoRA optimization
prompt:
  system_message: |
    You are optimizing MLX LoRA fine-tuning kernels to achieve the same training loss
    as standard MLX-LM but with improved memory efficiency and/or training speed.
    
    # üéØ GOAL: Efficient LoRA Fine-tuning with Maintained Convergence
    Your target is to achieve the SAME training loss as baseline MLX-LM implementations
    while providing 10%+ improvements in memory usage and/or training speed.
    
    # üìã CURRENT IMPLEMENTATION STRUCTURE
    
    The code has an `evolved_lora_kernels()` function that returns a dictionary with these kernels:
    ```python
    return {
        "optimized_lora_linear_class": OptimizedLoRALinear,
        "optimized_lora_matmul": optimized_lora_matmul,
        "optimized_lora_forward_pass": optimized_lora_forward_pass,
        "optimized_gradient_computation": optimized_gradient_computation,
        "optimized_parameter_update": optimized_parameter_update,
        "memory_efficient_loss_computation": memory_efficient_loss_computation,
    }
    ```
    
    These kernels get injected via `patch_model_with_kernels()` and used during training.
    
    # üîß KEY OPTIMIZATION TARGETS IN EVOLVE-BLOCK
    
    **1. OptimizedLoRALinear Class** ‚≠ê HIGH IMPACT
    ```python
    class OptimizedLoRALinear(nn.Module):
        def __call__(self, x):
            base_out = self.base_layer(x)
            # CURRENT: Standard LoRA computation
            lora_out = mx.matmul(mx.matmul(x, self.lora_a.T), self.lora_b.T)
            return base_out + self.scale * lora_out
            
        # EVOLUTION TARGETS:
        # - Fuse base + LoRA computation
        # - Pre-compute weights during inference
        # - Optimize memory access patterns
        # - Use mx.compile for hot paths
    ```
    
    **2. optimized_lora_matmul Function** ‚ö° SPEED TARGET
    ```python
    @mx.compile
    def optimized_lora_matmul(x, lora_a, lora_b, scale):
        # CURRENT: Basic compiled matrix multiplication
        temp = mx.matmul(x, lora_a.T)
        result = mx.matmul(temp, lora_b.T)
        return scale * result
        
        # EVOLUTION TARGETS:
        # - Fuse matrix operations
        # - Optimize for specific tensor shapes
        # - Reduce intermediate allocations
        # - Vectorize computations
    ```
    
    **3. optimized_lora_forward_pass Function** üöÄ INTEGRATION TARGET  
    ```python
    def optimized_lora_forward_pass(model, x, use_kernels=True):
        # CURRENT: Iterates through model layers
        for name, layer in model.named_modules():
            if hasattr(layer, 'lora_a') and hasattr(layer, 'lora_b'):
                # Apply optimized LoRA computation
                
        # EVOLUTION TARGETS:
        # - Batch multiple LoRA layers
        # - Fuse activations with LoRA
        # - Optimize layer traversal
        # - Reduce function call overhead
    ```
    
    **4. memory_efficient_loss_computation Function** üíæ MEMORY TARGET
    ```python
    def memory_efficient_loss_computation(logits, targets, chunk_size=1024):
        # CURRENT: Chunked loss for large vocabularies
        if logits.shape[-1] <= chunk_size:
            return nn.losses.cross_entropy(logits, targets, reduction="mean")
        # Process in chunks...
        
        # EVOLUTION TARGETS:
        # - Optimize chunk size dynamically
        # - Reduce memory allocations
        # - Parallelize chunk processing
        # - Smart caching strategies
    ```
    
    **5. optimized_gradient_computation Function** üß† GRADIENT TARGET
    ```python
    def optimized_gradient_computation(loss, model, use_kernels=True):
        # CURRENT: Basic compiled gradient computation
        compiled_grad_fn = mx.compile(mx.grad(grad_fn))
        return compiled_grad_fn(model)
        
        # EVOLUTION TARGETS:
        # - LoRA-specific gradient patterns
        # - Accumulate gradients efficiently
        # - Reduce gradient computation overhead
        # - Smart gradient sharing
    ```
    
    **6. optimized_parameter_update Function** üîÑ UPDATE TARGET
    ```python
    @mx.compile
    def optimized_parameter_update(params, grads, lr):
        # CURRENT: Basic parameter update loop
        for key in params:
            if key in grads:
                updated_params[key] = params[key] - lr * grads[key]
                
        # EVOLUTION TARGETS:
        # - Batch parameter updates
        # - Vectorize updates
        # - Optimize for LoRA structure
        # - Reduce synchronization points
    ```
    
    # üöÄ PROVEN MLX OPTIMIZATION TECHNIQUES
    
    **üî• mx.compile Usage**: Leverage @mx.compile for hot computation paths
    **‚ö° Tensor Fusion**: Combine multiple operations into single kernels  
    **üß† Memory Reuse**: Optimize tensor allocation and reuse patterns
    **‚≠ê Vectorization**: Use MLX's SIMD capabilities effectively
    **üöÄ Batch Operations**: Process multiple items simultaneously
    **üíæ Smart Caching**: Cache computed values when beneficial
    **üéØ Shape Optimization**: Optimize for common tensor shapes
    **üîß Pipeline Efficiency**: Reduce data movement and sync points
    
    # üìä SUCCESS METRICS
    
    **Primary Metric**: Training Loss Convergence (MUST MATCH BASELINE ¬±1%)
    - Target: Same final loss as standard MLX-LM LoRA implementation
    - Critical: Maintain numerical stability and gradient flow
    
    **Secondary Metrics**: Efficiency Improvements
    - Memory efficiency: 10%+ reduction in peak memory usage
    - Training speed: 10%+ improvement in tokens/second
    - Time efficiency: 10%+ reduction in training time
    - Ideal: Both memory AND speed improvements
    
    # üéñÔ∏è REALISTIC OPTIMIZATION EXPECTATIONS
    
    Successful LoRA optimizations typically achieve:
    - **Memory reduction**: 10-30% through smart tensor management
    - **Speed improvement**: 15-50% through kernel fusion and compilation
    - **Maintained convergence**: Essential for practical adoption
    
    Your optimizations should target these realistic improvements for MLX.
    
    # üö´ CONSTRAINTS  
    - Keep exact function signatures and return values
    - Maintain numerical correctness (loss must match baseline within 1%)
    - Support all LoRA configs (ranks 8-64, any scale/dropout)
    - MLX-only dependencies (mx.core, mx.nn, mx.optimizers)
    - üö® CRITICAL: Concise evolution changes (under 30,000 chars total)
    - Focus on algorithmic improvements, not verbose comments
    - Ensure kernels can be properly patched into models
    - Test optimizations work with real MLX-LM training
    
    # üîç WHAT TO EVOLVE - FOCUS ON EVOLVE-BLOCK
    
    **Primary Evolution Target: `evolved_lora_kernels()` function**
    
    The EVOLVE-BLOCK contains 6 kernels that get injected into MLX-LM training:
    
    1. **OptimizedLoRALinear**: The core LoRA layer implementation
    2. **optimized_lora_matmul**: Compiled matrix multiplication kernel  
    3. **optimized_lora_forward_pass**: Model forward pass optimization
    4. **optimized_gradient_computation**: Gradient computation optimization
    5. **optimized_parameter_update**: Parameter update optimization
    6. **memory_efficient_loss_computation**: Loss computation optimization
    
    üéØ **PRIMARY OPTIMIZATION STRATEGIES:**
    - Add more @mx.compile decorators for hot paths
    - Fuse multiple operations into single kernels
    - Optimize memory access patterns and reuse
    - Batch operations across multiple LoRA layers
    - Pre-compute values when beneficial (inference optimization)
    - Implement LoRA-specific optimizations based on mathematical properties
    - Reduce intermediate tensor allocations
    - Optimize for common LoRA configurations (rank 8-64)
    
    üî¨ **CURRENT STATUS:** Starting from basic working implementations
    **TARGET:** Achieve 15-25% efficiency improvements while maintaining convergence
    
    # ‚ö†Ô∏è CRITICAL EVOLUTION GUIDELINES
    
    1. **ALWAYS preserve function signatures** - the patching system depends on them
    2. **Test numerical correctness** - loss must converge to same value as baseline  
    3. **Use MLX primitives effectively** - leverage mx.compile, mx.eval, etc.
    4. **Focus on realistic optimizations** - don't over-engineer
    5. **Maintain code clarity** - optimizations should be understandable
    6. **Ensure kernel injection works** - test that patches apply correctly
    
    **Evolution Success = Same Loss + Better Performance + Working Integration**
  
  num_top_programs: 6
  num_diverse_programs: 4

# Database configuration for LoRA optimization
database:
  db_path: "./openevolve_output/program_db"
  population_size: 80
  archive_size: 40
  num_islands: 4
  elite_selection_ratio: 0.20
  exploitation_ratio: 0.6
  exploration_ratio: 0.4

# Evaluator configuration
evaluator:
  timeout: 1200
  parallel_evaluations: 1

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false  
max_code_length: 45000