# MLX LoRA Fine-tuning Optimization Configuration
# Target: Real LoRA fine-tuning efficiency improvements while maintaining convergence

max_iterations: 20  # Reduced for focused evolution
checkpoint_interval: 5
log_level: "INFO"

# LLM configuration
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.7
  secondary_model: "gemini-2.5-pro-preview-06-05"
  secondary_model_weight: 0.3
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.7  # Reduced for more focused changes
  top_p: 0.9
  max_tokens: 24000  # Reduced to focus on concise improvements
  timeout: 300

# SIMPLIFIED prompt targeting specific kernel improvements
prompt:
  system_message: |
    You are optimizing MLX LoRA kernels for better memory/speed while maintaining training convergence.
    
    # GOAL: 15%+ efficiency improvement, same training loss
    
    # CRITICAL RULES:
    1. ONLY modify code inside EVOLVE-BLOCK-START/END
    2. Keep ALL function signatures identical
    3. Focus on 1-2 kernels per evolution, not all at once
    4. Use @mx.compile for hot paths
    5. NO verbose comments - focus on actual optimizations
    
    # TARGET KERNELS (pick 1-2 per evolution):
    
    **OptimizedLoRALinear.__call__()** - Main computation bottleneck:
    ```python
    def __call__(self, x):
        base_out = self.base_layer(x)
        lora_out = mx.matmul(mx.matmul(x, self.lora_a.T), self.lora_b.T)
        return base_out + self.scale * lora_out
    ```
    OPTIMIZE: Fuse operations, reduce allocations, use mx.compile
    
    **optimized_lora_matmul()** - Core matrix ops:
    ```python
    @mx.compile
    def optimized_lora_matmul(x, lora_a, lora_b, scale):
        temp = mx.matmul(x, lora_a.T)
        result = mx.matmul(temp, lora_b.T)
        return scale * result
    ```
    OPTIMIZE: Better fusion, vectorization, memory layout
    
    **memory_efficient_loss_computation()** - Memory usage:
    ```python
    def memory_efficient_loss_computation(logits, targets, chunk_size=1024):
        if logits.shape[-1] <= chunk_size:
            return nn.losses.cross_entropy(logits, targets, reduction="mean")
        # chunk processing...
    ```
    OPTIMIZE: Dynamic chunking, parallel processing
    
    # PROVEN MLX OPTIMIZATIONS:
    - @mx.compile on computation-heavy functions
    - mx.fused ops to reduce intermediate tensors
    - Pre-compute constant expressions
    - Optimize tensor shapes and memory layout
    - Batch operations when possible
    
    # SUCCESS = Same loss + 15%+ speed OR memory improvement
    
    Make SMALL, FOCUSED changes. Test one optimization at a time.
  
  num_top_programs: 4  # Reduced for more focused evolution
  num_diverse_programs: 2

# Database configuration
database:
  db_path: "./openevolve_output/program_db"
  population_size: 40  # Reduced for faster iteration
  archive_size: 20
  num_islands: 2  # Reduced complexity
  elite_selection_ratio: 0.25
  exploitation_ratio: 0.7  # More exploitation for targeted improvements
  exploration_ratio: 0.3

# Evaluator configuration
evaluator:
  timeout: 600  # Reduced timeout
  parallel_evaluations: 1

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false  
max_code_length: 50000  # Reduced to encourage concise changes
