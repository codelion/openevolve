# MLX LoRA Fine-tuning Optimization Configuration
# Target: Real LoRA fine-tuning efficiency improvements while maintaining convergence

max_iterations: 40
checkpoint_interval: 5
log_level: "INFO"

# LLM configuration - use powerful models for LoRA optimization
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.7
  secondary_model: "gemini-2.5-pro-preview-06-05"
  secondary_model_weight: 0.3
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.8
  top_p: 0.9
  max_tokens: 32000
  timeout: 600

# Detailed prompt for LoRA optimization
prompt:
  system_message: |
    You are optimizing MLX LoRA fine-tuning implementations to achieve the same training loss
    as standard LoRA but with improved memory efficiency and/or training speed.
    
    # üéØ GOAL: Efficient LoRA Fine-tuning with Maintained Convergence
    Your target is to achieve the SAME training loss as baseline LoRA implementations
    while providing 10%+ improvements in memory usage and/or training speed.
    
    # üîß KEY OPTIMIZATION OPPORTUNITIES
    
    **1. LoRA Weight Pre-computation** ‚≠ê HIGH SUCCESS PROBABILITY
    ```python
    # Standard: 3 separate matrix multiplications per forward pass
    base_out = x @ base_weight.T
    lora_a_out = x @ lora_a.T  
    lora_b_out = lora_a_out @ lora_b.T
    result = base_out + scale * lora_b_out
    
    # Target: Pre-compute combined weights when beneficial
    if not self.training:  # During inference
        fused_weight = base_weight + scale * (lora_b @ lora_a)
        result = x @ fused_weight.T
    ```
    
    **2. Memory-Efficient Gradient Computation**
    ```python
    # Standard: Separate gradient computations
    grad_base = grad_output @ x.T
    grad_lora_b = grad_output @ lora_a_out.T  
    grad_lora_a = lora_b.T @ grad_output @ x.T
    
    # Target: Fused gradient computation to reduce memory allocations
    # Reuse intermediate tensors, optimize memory access patterns
    ```
    
    **3. Training Loop Optimization**
    ```python
    # Standard: Separate forward, loss, backward, update steps
    logits = model(inputs)
    loss = loss_fn(logits, targets)
    grads = compute_gradients(loss)
    optimizer.update(model, grads)
    
    # Target: Reduce kernel launches and memory overhead
    # Optimize for LoRA-specific gradient patterns
    ```
    
    **4. Multi-Layer LoRA Batch Processing**
    ```python
    # Standard: Apply LoRA to layers one by one
    for layer in layers:
        layer.q_proj = LoRALinear.from_linear(layer.q_proj)
        layer.v_proj = LoRALinear.from_linear(layer.v_proj)
    
    # Target: Batch LoRA operations across layers
    # Share computation, optimize memory utilization
    ```
    
    **5. Memory-Efficient Loss Computation**
    ```python
    # Standard: Full vocabulary materialization
    loss = cross_entropy(logits, targets)  # Memory: O(batch * seq * vocab)
    
    # Target: Chunked or online loss computation for large vocabularies
    # Reduce memory footprint during loss calculation
    ```
    
    # üöÄ PROVEN LORA OPTIMIZATION TECHNIQUES
    
    **Weight Fusion**: Pre-compute LoRA deltas when weights don't change
    **Gradient Reuse**: Optimize gradient computation patterns for LoRA structure  
    **Memory Access Optimization**: Better cache utilization during LoRA computations
    **Selective Computation**: Skip unnecessary computations based on LoRA rank
    **Training-Specific Optimizations**: Leverage LoRA's low-rank structure
    
    # üìä SUCCESS METRICS
    
    **Primary Metric**: Training Loss Convergence (MUST MATCH BASELINE ¬±1%)
    - Target: Same final loss as standard LoRA implementation
    - Critical: Maintain numerical stability and gradient flow
    
    **Secondary Metrics**: Efficiency Improvements
    - Memory efficiency: 10%+ reduction in peak memory usage
    - Training speed: 10%+ improvement in tokens/second
    - Ideal: Both memory AND speed improvements
    
    # üéñÔ∏è REAL-WORLD LORA OPTIMIZATION PATTERNS
    
    Successful LoRA optimizations typically achieve:
    - **Memory reduction**: 15-30% through weight fusion and gradient optimization
    - **Speed improvement**: 10-25% through reduced kernel launches and better memory access
    - **Maintained convergence**: Critical for practical adoption
    
    Your optimizations should target similar patterns adapted for MLX.
    
    # üö´ CONSTRAINTS
    - Keep the same function signatures and class interfaces
    - Maintain numerical correctness (final loss must match baseline within 1%)
    - Support all LoRA configurations (different ranks, scales, etc.)
    - No external dependencies beyond MLX
    - Focus on PRACTICAL optimizations that maintain convergence
    - üö® CRITICAL: Keep code changes MINIMAL and FOCUSED (under 40,000 chars)
    - NO verbose comments, examples, or redundant code
    - Use concise variable names and efficient implementations
    
    # üîç WHAT TO EVOLVE
    
    Focus on the `evolved_lora_kernels` function. The key operations to optimize:
    
    1. **OptimizedLoRALinear**: Improved LoRA linear layer implementation
    2. **optimized_lora_training_step**: More efficient training loop
    3. **optimized_multi_layer_lora_application**: Batch LoRA operations
    4. **memory_efficient_lora_loss**: Reduced memory loss computation
    5. **optimized_gradient_checkpointing_lora**: Memory-efficient checkpointing
    
    Evolve towards optimizations that provide real efficiency gains while maintaining
    the exact same training loss convergence as the baseline implementation.
  
  num_top_programs: 6
  num_diverse_programs: 4

# Database configuration for LoRA optimization
database:
  db_path: "./openevolve_output/program_db"
  population_size: 60
  archive_size: 30
  num_islands: 4
  elite_selection_ratio: 0.25
  exploitation_ratio: 0.7
  exploration_ratio: 0.3

# Evaluator configuration
evaluator:
  timeout: 1200  # Longer timeout for real LoRA training
  parallel_evaluations: 1

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false  
max_code_length: 50000
