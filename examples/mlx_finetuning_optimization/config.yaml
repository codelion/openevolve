# Configuration for MLX Fine-tuning Memory and Speed Optimization
# Streamlined for better evolution success

max_iterations: 100  # Increased for better exploration
checkpoint_interval: 10
log_level: "INFO"

# LLM configuration optimized for evolution
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.5  # Balanced mix
  secondary_model: "gemini-2.5-pro-preview-05-06" 
  secondary_model_weight: 0.5
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.9  # Higher for more exploration
  top_p: 0.95
  max_tokens: 8000  # Reduced for faster responses
  timeout: 600

# Concise MLX-focused prompt
prompt:
  system_message: |
    You are an expert MLX developer optimizing machine learning code for Apple Silicon.
    
    **CRITICAL MLX API RULES:**
    
    ❌ **FORBIDDEN (WILL ERROR):**
    - `mx.tree_flatten()`, `mx.tree_map()` - Don't exist in MLX
    - `grads.astype()` on dicts - Only works on mx.array
    - `mx.value_and_grad(fn, has_aux=True)` - has_aux not supported
    - `float(tuple_value)` - Always extract scalar first
    - `mx.eval(loss)[0]` if eval returns None
    
    ✅ **REQUIRED PATTERNS:**
    ```python
    # Gradient processing
    for name, grad in grads.items():
        if isinstance(grad, mx.array):
            grad = grad.astype(mx.float32)
    
    # Safe loss extraction  
    loss_value, grads = mx.value_and_grad(loss_fn)(model)
    # loss_fn must return ONLY loss, not tuple
    
    # Safe evaluation
    def safe_eval(tensor, fallback=2.0):
        try:
            result = mx.eval(tensor)
            return float(result) if result is not None else fallback
        except:
            return fallback
    
    # Safe array indexing
    if batch.ndim >= 2:
        inputs, targets = batch[:, :-1], batch[:, 1:]
    else:
        inputs, targets = batch[:-1], batch[1:]
    ```
    
    **GOALS:**
    - Reduce memory usage 20-40%
    - Improve speed 10-30% 
    - Keep loss in range 0.1-10.0
    - Use defensive programming (check types, handle None)
    - Never use zero/NaN as loss fallbacks
    
    **FOCUS:** Evolve gradient accumulation and memory-efficient patterns for MLX fine-tuning.

  num_top_programs: 3
  num_diverse_programs: 2
  use_template_stochasticity: true

# Database configuration for better evolution
database:
  population_size: 50  # Smaller for faster iterations
  archive_size: 20
  num_islands: 4  # More diversity
  elite_selection_ratio: 0.15
  exploitation_ratio: 0.5  # More exploration
  exploration_ratio: 0.5

# Evaluator configuration
evaluator:
  timeout: 300  # Faster evaluation
  cascade_evaluation: true
  cascade_thresholds: [0.3, 0.6]  # More permissive
  parallel_evaluations: 1
  use_llm_feedback: false

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false
max_code_length: 20000  # Smaller for focused changes
