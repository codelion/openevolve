# Hierarchical Evolution Integration Status

**Last Updated**: 2025-11-17

## Summary

The hierarchical abstraction layer system has been implemented but requires additional integration work to fully connect with the worker process execution model.

## âœ… Completed

### 1. Core Hierarchical Components
- âœ… Five-layer abstraction system (`layers.py`)
- âœ… Evolutionary Memory Graph (`emg.py`)
- âœ… Context compilation (`context.py`)
- âœ… Tiered model selection (`model_tiers.py`)
- âœ… Layer transitions (`transitions.py`)
- âœ… Insight extraction (`insights.py`)
- âœ… Orchestrator (`orchestrator.py`)

### 2. Configuration Support
- âœ… `HierarchicalConfig` in `config.py`
- âœ… Tier model configuration (tier0-tier3)
- âœ… Layer transition triggers
- âœ… EMG and insight settings

### 3. Multi-API Support
- âœ… Per-model `api_base` configuration
- âœ… Automatic API key detection based on URL
  - `moonshot.cn` â†’ `KIMI_API_KEY`
  - `bigmodel.cn`/`z.ai` â†’ `GLM_API_KEY`
  - `openai.com` â†’ `OPENAI_API_KEY`
- âœ… Applied to all tier models in `load_config()`

### 4. Controller Integration
- âœ… Controller initializes `HierarchicalOrchestrator`
- âœ… Orchestrator has access to database and config
- âœ… Orchestrator creates tiered model selector
- âœ… Method `get_ensemble_for_iteration()` implemented

### 5. Bug Fixes
- âœ… Fixed circular import (`NodeType`, `TransitionTriggers`)
- âœ… Fixed evaluator return format (now returns `EvaluationResult`)
- âœ… Added problem-specific `system_message` to all configs

## âŒ Not Completed - Critical Gap

### Worker Process Integration

**Problem**: The hierarchical orchestrator's tiered ensembles are not used in worker processes.

**Current Behavior**:
1. Controller creates `HierarchicalOrchestrator`
2. Orchestrator can determine which tier/ensemble to use for each iteration
3. **But** worker processes always use `config.llm.models` (the base models)
4. Tiered models (tier0-tier3) are never actually used for LLM generation

**Code Flow**:
```
Controller (has orchestrator)
  â†“
ProcessParallelController (no orchestrator access)
  â†“
_submit_iteration(iteration, ...)
  â†“
_run_iteration_worker(iteration, db_snapshot, ...)
  â†“
_lazy_init_worker_components()
  â†“
_worker_llm_ensemble = LLMEnsemble(config.llm.models)  â† Always uses base models!
```

**Evidence**:
- `openevolve/process_parallel.py:105` - Worker creates ensemble from `config.llm.models`
- `openevolve/process_parallel.py:189` - Worker uses `_worker_llm_ensemble` for generation
- `openevolve/hierarchy/orchestrator.py:175` - `get_ensemble_for_iteration()` exists but is never called
- `grep -r "get_ensemble_for_iteration"` - Only found in orchestrator.py, not called anywhere

**Impact**:
- Hierarchical evolution appears to run (no errors)
- But all iterations use the same models
- Cost optimization is not realized
- Strategic reasoning at higher layers doesn't happen
- The system degenerates to standard evolution with overhead

## ðŸ”§ Required Fix

### Solution Design

To properly integrate tiered models, we need to:

**Option 1: Pass Tier Models Per Iteration** (Recommended)
1. Modify `ProcessParallelController.__init__()` to accept `hierarchical_orchestrator`
2. In `_submit_iteration()`:
   ```python
   # Get models for this iteration
   if self.hierarchical_orchestrator:
       tier_ensemble = self.hierarchical_orchestrator.get_ensemble_for_iteration(iteration)
       tier_models = [model.to_dict() for model in tier_ensemble.models]
       db_snapshot["tier_models"] = tier_models
   ```
3. In `_run_iteration_worker()`:
   ```python
   # Use tier models if provided
   if "tier_models" in db_snapshot and db_snapshot["tier_models"]:
       models = [LLMModelConfig(**m) for m in db_snapshot["tier_models"]]
   else:
       models = _worker_config.llm.models

   _worker_llm_ensemble = LLMEnsemble(models)
   ```

**Option 2: Recreate Orchestrator in Worker**
- Pass hierarchical config to worker
- Worker creates its own orchestrator
- Worker calls `get_ensemble_for_iteration(iteration)`
- More overhead but cleaner separation

**Option 3: Pre-compute Tier Assignments**
- At evolution start, pre-compute which tier for each iteration
- Pass tier assignment table to workers
- Workers look up their tier and use corresponding models
- Inflexible but simple

### Implementation Steps

1. **Modify ProcessParallelController**:
   - Add `hierarchical_orchestrator` parameter to `__init__()`
   - Store as `self.hierarchical_orchestrator`

2. **Modify controller.py initialization**:
   ```python
   self.parallel_controller = ProcessParallelController(
       self.config,
       self.evaluation_file,
       self.database,
       self.evolution_tracer,
       file_suffix=self.config.file_suffix,
       hierarchical_orchestrator=self.hierarchical_orchestrator,  # â† Add this
   )
   ```

3. **Modify _submit_iteration**:
   - Call `orchestrator.get_ensemble_for_iteration(iteration)`
   - Extract model configs from ensemble
   - Add to `db_snapshot["tier_models"]`

4. **Modify _lazy_init_worker_components**:
   - Check for `tier_models` in snapshot (passed as global)
   - If present, use those models
   - Otherwise fall back to `config.llm.models`

5. **Update _run_iteration_worker signature**:
   - Current: `_run_iteration_worker(iteration, db_snapshot, parent_id, inspiration_ids)`
   - Option A: Add `tier_models` parameter
   - Option B: Put in `db_snapshot` (cleaner, already passing dict)

### Testing

After implementation, verify:
1. Log which models are used for each iteration
2. Confirm tier progression (Tier 0 â†’ Tier 1 â†’ Tier 2 â†’ Tier 3)
3. Check API calls go to correct endpoints
4. Verify cost is reduced vs using premium models for all

## ðŸ“‹ Temporary Workaround

Until proper integration is complete, users can manually set the `llm.models` to be the Tier 0 models they want to use throughout evolution. This provides:
- âœ… Consistent model usage
- âœ… API key detection works
- âœ… Cost predictability
- âŒ No hierarchical reasoning benefits
- âŒ No automatic tier escalation

## ðŸ“ Documentation Needed

Once fixed, update:
- `/docs/MULTI_API_SETUP.md` - Remove any caveats about tier integration
- `/examples/*/README.md` - Add actual vs expected tier progression logs
- Add troubleshooting section for verifying tier usage
- Create example log output showing tier transitions

## ðŸ” Verification Commands

Check if hierarchical tiers are working:

```bash
# Run with verbose logging
python run_hierarchical.py --config config_multi_api.yaml --iterations 10 2>&1 | grep -i "tier\|ensemble\|model"

# Should see logs like:
# "Using code_details ensemble in normal phase (iteration 1)"
# "Using implementation_patterns ensemble in normal phase (iteration 7)"

# Check API calls
python run_hierarchical.py --config config_multi_api.yaml --iterations 5 2>&1 | grep "HTTP Request: POST"

# Should see different API endpoints:
# POST https://open.bigmodel.cn/...  (GLM for Tier 0-2)
# POST https://api.moonshot.cn/...   (KIMI for Tier 3)
```

## ðŸŽ¯ Priority

**High Priority** - This is the core value proposition of hierarchical evolution. Without it, the system is just standard evolution with extra overhead.

## ðŸ“ž Contact

For questions or to contribute the fix, please open an issue or PR on GitHub.
